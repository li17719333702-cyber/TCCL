"""
ç»¼åˆåŸºå‡†æµ‹è¯•è„šæœ¬

å¯¹æ¯”14ä¸ªæ–¹æ³•ï¼š
- ä¼ ç»Ÿæ–¹æ³•ï¼ˆ3ä¸ªï¼‰ï¼šRaw+UMAP, Handcrafted Features, PCA+K-Means
- æ·±åº¦èšç±»ï¼ˆ3ä¸ªï¼‰ï¼šDEC, JULE, SCAN
- å¯¹æ¯”å­¦ä¹ -ç»å…¸ï¼ˆ3ä¸ªï¼‰ï¼šSimCLR, MoCo, BYOL
- å¯¹æ¯”å­¦ä¹ -SOTAï¼ˆ4ä¸ªï¼‰ï¼šSimSiam, TS2Vec, VICReg, TimesNet
- æœ¬æ–‡æ–¹æ³•ï¼ˆ1ä¸ªï¼‰ï¼šTCCL

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•ä¸ªæ•°æ®é›†
    python benchmark.py --dataset CWRU --data_root E:/AI/CWRU-dataset-main/48k007 --epochs 50
    python benchmark.py --dataset SEU --data_root E:/AI/Mechanical-datasets-master/dataset --epochs 50
    python benchmark.py --dataset MFPT --data_root E:/AI/MFPT-Fault-Data-Sets-20200227T131140Z-001/MFPT/MFPT --epochs 50
    
    # æ‰¹é‡è¿è¡Œæ‰€æœ‰æ•°æ®é›†ï¼ˆä¸æ˜¾ç¤ºå›¾è¡¨ï¼Œè‡ªåŠ¨ä¿å­˜ï¼‰
    python benchmark.py --all --epochs 50
    
    # å•æ•°æ®é›†ä½†ä¸æ˜¾ç¤ºå›¾è¡¨
    python benchmark.py --dataset CWRU --data_root E:/AI/CWRU-dataset-main/48k007 --epochs 50 --no_show
"""

import argparse
import copy
from pathlib import Path
from collections import defaultdict
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

import os
import random
import numpy as np
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm
import umap
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score

# å¯¼å…¥æ•°æ®é›†
from datasets import CWRUDataset, SEUDataset, MFPTDataset

# æ•°æ®å¢å¼ºå™¨ï¼ˆæœ¬åœ°å®ç°ï¼Œä¸æ•°æ®é›†æ¥å£åŒ¹é…ï¼‰
class SignalAugmentation:
    def __init__(self, noise_level: float = 0.05, scale_range=(0.9, 1.1)):
        self.noise_level = noise_level
        self.scale_range = scale_range

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        scale = torch.empty(1, device=x.device).uniform_(self.scale_range[0], self.scale_range[1]).item()
        noise = torch.randn_like(x) * self.noise_level
        return x * scale + noise

# å¯¼å…¥åŸºçº¿æ¨¡å‹ï¼ˆæ–°çš„æ¨¡å—åŒ–ç»“æ„ï¼‰
from models.baselines import (
    # åŸºç¡€ç»„ä»¶
    FeatureExtractor,
    # ä¼ ç»Ÿæ–¹æ³•
    RawUMAPModel,
    HandcraftedFeaturesModel,
    PCAKMeansModel,
    # æ·±åº¦èšç±»
    DECModel,
    JULEModel,
    SCANModel,
    # å¯¹æ¯”å­¦ä¹ -ç»å…¸
    SimCLRModel,
    MoCoModel,
    BYOLModel,
    # å¯¹æ¯”å­¦ä¹ -SOTA
    SimSiamModel,
    TS2VecModel,
    VICRegModel,
    TimesNetModel,
)

# å¯¼å…¥å·¥å…·
from utils.benchmark_utils import (
    train_model,
    extract_features_from_loader
)


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        # ç»Ÿä¸€éšæœºç§å­
        try:
            set_random_seed(config.get('seed', 42))
        except Exception:
            pass
        
        self.models = {}
        self.results = defaultdict(dict)
        self.training_history = defaultdict(list)
        
        print("=" * 80)
        print("ğŸš€ COMPREHENSIVE BENCHMARK FRAMEWORK")
        print("=" * 80)
        print(f"Dataset: {config['dataset']}")
        print(f"Device: {self.device}")
        print(f"Epochs: {config['epochs']}")
        print(f"Batch Size: {config['batch_size']}")
        print("=" * 80)
    
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®é›†"""
        print("\nğŸ“‚ Loading dataset...")
        
        # æ•°æ®å¢å¼º
        augmentor = SignalAugmentation(
            noise_level=self.config.get('noise_level', 0.05),
            scale_range=self.config.get('scale_range', (0.9, 1.1))
        )
        
        # æ•°æ®é›†ç±»æ˜ å°„
        dataset_classes = {
            'CWRU': CWRUDataset,
            'SEU': SEUDataset,
            'MFPT': MFPTDataset
        }
        
        if self.config['dataset'] not in dataset_classes:
            raise ValueError(f"Unknown dataset: {self.config['dataset']}")
        
        DatasetClass = dataset_classes[self.config['dataset']]
        
        # è®­ç»ƒé›†ï¼ˆå¸¦å¢å¼ºï¼‰
        train_dataset = DatasetClass(
            root_dir=self.config['data_root'],
            window_size=self.config['window_size'],
            step_size=self.config['step_size'],
            mode='full',
            augmentor=augmentor
        )
        
        # è¯„ä¼°é›†ï¼ˆä¸å¸¦å¢å¼ºï¼‰
        eval_dataset = DatasetClass(
            root_dir=self.config['data_root'],
            window_size=self.config['window_size'],
            step_size=self.config['step_size'],
            mode='full',
            augmentor=None
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config.get('num_workers', 0),
            drop_last=True
        )
        
        self.eval_loader = DataLoader(
            eval_dataset,
            batch_size=256,
            shuffle=False,
            num_workers=self.config.get('num_workers', 0)
        )
        
        self.class_names = {v: k for k, v in train_dataset.LABEL_MAP.items()}
        self.n_classes = len(self.class_names)
        
        print(f"âœ“ Train samples: {len(train_dataset)}")
        print(f"âœ“ Eval samples: {len(eval_dataset)}")
        print(f"âœ“ Classes: {list(self.class_names.values())}")
        
        # åˆå§‹åŒ–æ¯æ¨¡å‹æ¸©åº¦è¦†ç›–ï¼ˆå¦‚æœªæä¾›åˆ™ä½¿ç”¨åˆç†é»˜è®¤ï¼‰
        # å…è®¸ç”¨æˆ·é€šè¿‡ config['temperature_overrides'] è¦†ç›–
        default_overrides = {
            'TCCL': 0.01,                # æ¨¡æ¿ç›¸å…³å“åº”ï¼Œè¾ƒä½ Ï„
            'SimCLR': 0.07,              # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå¼å¸¸ç”¨ Ï„
            'MoCo': 0.07,                # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå¼å¸¸ç”¨ Ï„
            'SimSiam': 0.05,
            'TS2Vec': 0.07,
            'TimesNet': 0.07,
        }
        user_overrides = self.config.get('temperature_overrides', {}) or {}
        self.temperature_overrides = {**default_overrides, **user_overrides}
    
    def create_models(self):
        """åˆ›å»ºæ‰€æœ‰æ¨¡å‹"""
        print("\nğŸ”¨ Creating models...")
        
        # åŸºç¡€ç‰¹å¾æå–å™¨
        feature_extractor_base = FeatureExtractor(in_channels=1, out_channels=64)
        
        # å®šä¹‰æ‰€æœ‰æ¨¡å‹
        models_config = {
            # === ä¼ ç»Ÿæ–¹æ³• ===
            'Raw+UMAP': RawUMAPModel(),
            'Handcrafted Features': HandcraftedFeaturesModel(),
            'PCA+K-Means': PCAKMeansModel(n_components=64),
            
            # === æ·±åº¦èšç±» ===
            'DEC': DECModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                n_clusters=self.n_classes
            ),
            'JULE': JULEModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                n_clusters=self.n_classes
            ),
            'SCAN': SCANModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                n_clusters=self.n_classes
            ),
            
            # === å¯¹æ¯”å­¦ä¹ ï¼ˆç»å…¸ 2020ï¼‰ ===
            'SimCLR': SimCLRModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                projection_dim=128,
                hidden_dim=2048
            ),
            'MoCo': MoCoModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                projection_dim=128,
                hidden_dim=2048,
                queue_size=min(65536, len(self.train_loader.dataset)),
                momentum=0.999
            ),
            # 'BYOL': BYOLModel(
            #     feature_extractor=copy.deepcopy(feature_extractor_base),
            #     projection_dim=128,
            #     hidden_dim=2048
            # ),
            
            # === å¯¹æ¯”å­¦ä¹ ï¼ˆSOTA 2021-2023ï¼‰ ===
            'SimSiam': SimSiamModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                projection_dim=128,
                hidden_dim=2048
            ),
            'TS2Vec': TS2VecModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                projection_dim=128
            ),
            # 'VICReg': VICRegModel(
            #     feature_extractor=copy.deepcopy(feature_extractor_base),
            #     projection_dim=128,
            #     hidden_dim=2048
            # ),
            'TimesNet': TimesNetModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                projection_dim=128
            ),
            
            # === æœ¬æ–‡æ–¹æ³•ï¼ˆå¯¹æ¯”å­¦ä¹ é£æ ¼å®ç°ï¼‰===
            'TCCL': __import__('models.baselines', fromlist=['TCCLModel', 'FeatureExtractor']).contrastive.TCCLModel(
                feature_extractor=copy.deepcopy(feature_extractor_base),
                kernel_width=3,
                temperature=self.config['temperature']
            ),
        }
        
        # æ³¨å†Œæ¨¡å‹
        self.models = models_config
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print("\nğŸ“‹ Registered Models:")
        print("-" * 80)
        
        categories = {
            'Traditional': ['Raw+UMAP', 'Handcrafted Features', 'PCA+K-Means'],
            'Deep Clustering': ['DEC', 'JULE', 'SCAN'],
            'Contrastive (2020)': ['SimCLR', 'MoCo', 'BYOL'],
            'Contrastive (SOTA)': ['SimSiam', 'TS2Vec', 'VICReg', 'TimesNet'],
            'TCCL (Ours)': ['TCCL']
        }
        
        for category, methods in categories.items():
            print(f"\n{category}:")
            for method in methods:
                if method in self.models:
                    model = self.models[method]
                    num_params = sum(p.numel() for p in model.parameters())
                    print(f"  âœ“ {method:<25s} {num_params:>12,} params")
    
    def train_all_models(self):
        """è®­ç»ƒæ‰€æœ‰éœ€è¦è®­ç»ƒçš„æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("ğŸ“ TRAINING PHASE")
        print("=" * 80)
        
        for name, model in self.models.items():
            if not model.needs_training:
                print(f"\nâ­ï¸  Skipping {name} (no training required)")
                continue
            
            print(f"\n{'='*60}")
            print(f"ğŸš€ Training {name} [{model.model_type}]")
            print(f"{'='*60}")
            
            # æ¯æ¨¡å‹æ¸©åº¦è¦†ç›–
            temp = self.temperature_overrides.get(name, self.config['temperature'])

            history = train_model(
                model=model,
                train_loader=self.train_loader,
                device=self.device,
                epochs=self.config['epochs'],
                learning_rate=self.config['learning_rate'],
                temperature=temp,
                verbose=True
            )
            
            self.training_history[name] = history
            print(f"âœ“ {name} training completed")
    
    def _prefit_pca_on_eval(self):
        """åœ¨è¯„ä¼°é›†ä¸Šé¢„æ‹Ÿåˆ PCAï¼Œä»¥é¿å…æŒ‰ batch æ‹Ÿåˆå¯¼è‡´çš„åç½®ã€‚"""
        if 'PCA+K-Means' not in self.models:
            return
        model = self.models['PCA+K-Means']
        if not hasattr(model, 'fit_pca'):
            return
        print("\nğŸ§® Prefitting PCA on full eval set...")
        all_signals = []
        with torch.no_grad():
            for x, _, _ in self.eval_loader:
                # x: [B, 1, L] -> [B, L]
                x_flat = x.view(x.size(0), -1).cpu().numpy()
                all_signals.append(x_flat)
        if all_signals:
            all_signals = np.concatenate(all_signals, axis=0)
            try:
                model.fit_pca(all_signals, verbose=True)
                print("  âœ“ PCA prefitted on eval set")
            except Exception as e:
                print(f"  âŒ PCA prefit failed: {e}")

    @staticmethod
    def _zscore_features(features: np.ndarray) -> np.ndarray:
        """å¯¹ç‰¹å¾è¿›è¡Œ z-score æ ‡å‡†åŒ–ï¼ˆæŒ‰ç‰¹å¾ç»´åº¦ï¼‰ï¼Œé¿å…å°ºåº¦å½±å“èšç±»ã€‚"""
        mean = features.mean(axis=0, keepdims=True)
        std = features.std(axis=0, keepdims=True)
        std[std < 1e-8] = 1.0
        return (features - mean) / std

    def evaluate_all_models(self):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("ğŸ“Š EVALUATION PHASE")
        print("=" * 80)
        # æ³¨ï¼šæŒ‰ç”¨æˆ·è¦æ±‚ï¼Œæ¢å¤ä¸ºä¸åœ¨è¯„ä¼°å‰è¿›è¡Œå…¨é‡ PCA é¢„æ‹Ÿåˆ
        # PCA å°†åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨ PCAKMeansModel.extract_features() æ—¶æŒ‰æ—§é€»è¾‘è‡ªåŠ¨æ‹Ÿåˆ
        
        for name, model in self.models.items():
            print(f"\nğŸ“Š Evaluating {name}...")
            
            try:
                # æå–ç‰¹å¾
                print("  - Extracting features...")
                features, labels = extract_features_from_loader(
                    model, self.eval_loader, self.device
                )
                # ç»Ÿä¸€ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæŒ‰æ¨¡å‹ï¼‰
                features = self._zscore_features(features)
                
                # ä½¿ç”¨ single_tccl.py é£æ ¼ï¼šä¸åšL2ï¼›UMAP->2Dï¼›DBSCANï¼›å™ªå£°KNNå›å¡«
                print("  - Computing metrics with UMAP+DBSCAN (single-style, no L2)...")
                eval_res = evaluate_clustering_single_style(features, labels)
                metrics = {
                    'accuracy_Nto1': eval_res['acc'],
                    'ari': eval_res['ari'],
                    'nmi': eval_res['nmi'],
                    'silhouette': eval_res['sil'],
                    'n_clusters': eval_res['n_clusters'],
                    'separation_ratio': float('nan'),
                    'davies_bouldin': float('nan'),
                    'calinski_harabasz': float('nan'),
                }
                pred_labels = eval_res['pred_labels']
                features_2d = eval_res['features_reduced']
                
                # ä¿å­˜ç»“æœ
                self.results[name] = {
                    'features': features,
                    'features_clean': features,  # ä¸singleå¯¹é½ï¼Œä¸åšL2
                    'metrics': metrics,
                    'pred_labels': pred_labels,
                    'features_2d': features_2d,
                    'labels': labels
                }
                
                print(f"  âœ“ {name}: Acc={metrics['accuracy_Nto1']:.4f}, "
                      f"ARI={metrics['ari']:.4f}, NMI={metrics['nmi']:.4f}, "
                      f"Sil={metrics['silhouette']:.4f}")
                
            except Exception as e:
                print(f"  âŒ Failed to evaluate {name}: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def print_comparison_table(self):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
        print("\n" + "=" * 140)
        print("ğŸ“Š COMPREHENSIVE PERFORMANCE COMPARISON")
        print("=" * 140)
        
        categories = {
            'Traditional Methods': ['Raw+UMAP', 'Handcrafted Features', 'PCA+K-Means'],
            'Deep Clustering': ['DEC', 'JULE', 'SCAN'],
            'Contrastive Learning (2020)': ['SimCLR', 'MoCo', 'BYOL'],
            'Contrastive Learning (SOTA 2021-2023)': ['SimSiam', 'TS2Vec', 'VICReg', 'TimesNet'],
            'TCCL (Ours)': ['TCCL']
        }
        
        for category, methods in categories.items():
            print(f"\n{category}:")
            print("-" * 140)
            print(f"{'Method':<25} {'Accâ†‘':>8} {'ARIâ†‘':>8} {'NMIâ†‘':>8} {'Silâ†‘':>8} "
                  f"{'SepRâ†‘':>8} {'DBâ†“':>8} {'CHâ†‘':>9}")
            print("-" * 140)
            
            for method in methods:
                if method in self.results:
                    m = self.results[method]['metrics']
                    print(f"{method:<25} {m['accuracy_Nto1']:>8.4f} {m['ari']:>8.4f} "
                          f"{m['nmi']:>8.4f} {m['silhouette']:>8.4f} "
                          f"{m['separation_ratio']:>8.4f} {m['davies_bouldin']:>8.4f} "
                          f"{m['calinski_harabasz']:>9.1f}")
        
        print("=" * 140)
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        print("\nğŸ¨ Generating visualizations...")
        
        # 1. è®­ç»ƒæ›²çº¿
        if self.training_history:
            self._plot_training_curves()
        
        # 2. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        self._plot_performance_bars()
        
        # 3. é›·è¾¾å›¾
        self._plot_radar_chart()
        
        # 4. èšç±»å¯è§†åŒ–
        if self.config.get('visualize_clusters', True):
            self._plot_clustering_results()
    
    def _plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ - ä¸¤ç§æ–¹å¼ï¼š1) æ‰€æœ‰æŸå¤±åœ¨ä¸€å¼ å›¾ä¸Š 2) 3*3å­å›¾å¸ƒå±€"""
        
        # === æ–¹å¼1: æ‰€æœ‰æŸå¤±ç»˜åˆ¶åœ¨ä¸€å¼ å›¾ä¸Š ===
        fig1, ax1 = plt.subplots(figsize=(12, 8))
        
        # å®šä¹‰é¢œè‰²å’Œçº¿å‹
        colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', 
                 '#1abc9c', '#e67e22', '#34495e', '#16a085', '#c0392b',
                 '#8e44ad', '#27ae60', '#d35400', '#7f8c8d']
        linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '-', '--', '-.', ':', '-', '--']
        
        for idx, (name, history) in enumerate(self.training_history.items()):
            color = colors[idx % len(colors)]
            linestyle = linestyles[idx % len(linestyles)]
            ax1.plot(history, linewidth=2.5, color=color, linestyle=linestyle, 
                    label=name, marker='o', markersize=3, markevery=max(1, len(history)//10))
        
        ax1.set_xlabel('Epoch', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Loss', fontsize=14, fontweight='bold')
        ax1.set_title('Training Curves - All Methods', fontsize=16, fontweight='bold')
        ax1.legend(loc='best', fontsize=10, framealpha=0.9, ncol=2)
        ax1.grid(True, alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        # plt.savefig('benchmark_training_curves_combined.png', dpi=300, bbox_inches='tight')
        print("  âœ“ Saved: benchmark_training_curves_combined.png")
        if not self.config.get('no_show', False):
            plt.show()
        plt.close(fig1)
        
        # === æ–¹å¼2: 3*3å­å›¾å¸ƒå±€ ===
        fig2, axes = plt.subplots(3, 3, figsize=(18, 15))
        axes = axes.flatten()
        
        # è·å–æ‰€æœ‰æ–¹æ³•åç§°
        method_names = list(self.training_history.keys())
        
        # ä¸ºæ¯ä¸ªå­å›¾ç»˜åˆ¶å¯¹åº”çš„æŸå¤±æ›²çº¿
        for idx in range(9):  # 3*3 = 9ä¸ªå­å›¾
            ax = axes[idx]
            
            if idx < len(method_names):
                name = method_names[idx]
                history = self.training_history[name]
                ax.plot(history, linewidth=2.5, color='#2ecc71', marker='o', 
                       markersize=4, markevery=max(1, len(history)//10))
                ax.set_xlabel(' ', fontsize=11)
                ax.set_ylabel(' ', fontsize=11)
                ax.set_title(f'{name}', fontsize=12, fontweight='bold')
                ax.grid(True, alpha=0.3, linestyle='--')
            else:
                # éšè—å¤šä½™çš„å­å›¾
                ax.axis('off')
        
        # plt.suptitle('Training Curves - 3Ã—3 Subplot Layout', 
        #             fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        # plt.savefig('benchmark_training_curves_subplots.png', dpi=300, bbox_inches='tight')
        print("  âœ“ Saved: benchmark_training_curves_subplots.png")
        if not self.config.get('no_show', False):
            plt.show()
        plt.close(fig2)
    
    def _plot_performance_bars(self):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        axes = axes.flatten()
        
        metric_names = {
            'accuracy_Nto1': 'Clustering Accuracy (N-to-1)',
            'ari': 'Adjusted Rand Index',
            'nmi': 'Normalized Mutual Information',
            'silhouette': 'Silhouette Coefficient'
        }
        
        metric_keys = ['accuracy_Nto1', 'ari', 'nmi', 'silhouette']
        
        for idx, metric_key in enumerate(metric_keys):
            ax = axes[idx]
            
            methods = []
            values = []
            colors = []
            
            for method, result in self.results.items():
                methods.append(method)
                values.append(result['metrics'][metric_key])
                
                # æ ¹æ®æ–¹æ³•ç±»åˆ«ç€è‰²
                if method in ['Raw+UMAP', 'Handcrafted Features', 'PCA+K-Means']:
                    colors.append('#95a5a6')
                elif method in ['DEC', 'JULE', 'SCAN']:
                    colors.append('#3498db')
                elif method in ['SimCLR', 'MoCo', 'BYOL']:
                    colors.append('#9b59b6')
                elif method in ['SimSiam', 'TS2Vec', 'VICReg', 'TimesNet']:
                    colors.append('#2ecc71')
                else:  # TCCL
                    colors.append('#e74c3c')
            
            bars = ax.bar(range(len(methods)), values, color=colors,
                         edgecolor='black', linewidth=1.5)
            
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:.3f}', ha='center', va='bottom',
                       fontsize=9, fontweight='bold')
            
            ax.set_ylabel(metric_names[metric_key], fontsize=11, fontweight='bold')
            ax.set_title(metric_names[metric_key], fontsize=12, fontweight='bold')
            ax.set_xticks(range(len(methods)))
            ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=8)
            ax.grid(True, alpha=0.3, axis='y')
            ax.set_ylim(0, max(values) * 1.15)
        
        plt.suptitle('Performance Comparison Across All Methods',
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        # plt.savefig('benchmark_performance_bars.png', dpi=300, bbox_inches='tight')
        print("  âœ“ Saved: benchmark_performance_bars.png")
        if not self.config.get('no_show', False):
            plt.show()
        plt.close(fig)
    
    def _plot_radar_chart(self):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        key_methods = ['Handcrafted Features', 'DEC', 'SimCLR', 'TS2Vec', 'TimesNet', 'TCCL']
        metrics_to_plot = ['Acc', 'ARI', 'NMI', 'Sil']
        
        angles = np.linspace(0, 2 * np.pi, len(metrics_to_plot), endpoint=False).tolist()
        angles += angles[:1]
        
        colors = ['#95a5a6', '#3498db', '#9b59b6', '#16a085', '#2ecc71', '#e74c3c']
        
        for idx, method in enumerate(key_methods):
            if method in self.results:
                metrics = self.results[method]['metrics']
                values = [
                    metrics['accuracy_Nto1'],
                    metrics['ari'],
                    metrics['nmi'],
                    metrics['silhouette']
                ]
                values += values[:1]
                
                ax.plot(angles, values, 'o-', linewidth=2, label=method, color=colors[idx])
                ax.fill(angles, values, alpha=0.15, color=colors[idx])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_to_plot, fontsize=12)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)
        ax.set_title('Performance Comparison (Radar Chart)',
                    fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        # plt.savefig('benchmark_radar_chart.png', dpi=300, bbox_inches='tight')
        print("  âœ“ Saved: benchmark_radar_chart.png")
        if not self.config.get('no_show', False):
            plt.show()
        plt.close(fig)
    
    def _plot_clustering_results(self):
        """ç»˜åˆ¶èšç±»ç»“æœï¼ˆçœŸå®æ ‡ç­¾ vs èšç±»ç»“æœï¼‰"""
        n_methods = len(self.results)
        n_cols = min(3, n_methods)  # å‡å°‘åˆ—æ•°ï¼Œå› ä¸ºæ¯ä¸ªæ–¹æ³•è¦æ˜¾ç¤º2ä¸ªå­å›¾
        n_rows = (n_methods + n_cols - 1) // n_cols
        
        # æ¯ä¸ªæ–¹æ³•æ˜¾ç¤º2ä¸ªå­å›¾ï¼ˆçœŸå®æ ‡ç­¾ + èšç±»ç»“æœï¼‰ï¼Œæ‰€ä»¥å®é™…åˆ—æ•°è¦ x2
        fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(6*n_cols, 4*n_rows))
        
        # å¤„ç†axesçš„ç»´åº¦
        if n_methods == 1:
            axes = axes.reshape(1, -1)
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        
        for idx, (method, result) in enumerate(self.results.items()):
            if result['features_2d'] is None:
                continue
            
            row = idx // n_cols
            col_base = (idx % n_cols) * 2
            
            ax_true = axes[row, col_base]      # å·¦ä¾§ï¼šçœŸå®æ ‡ç­¾
            ax_pred = axes[row, col_base + 1]  # å³ä¾§ï¼šèšç±»ç»“æœ
            
            features_2d = result['features_2d']
            true_labels = result['labels']
            pred_labels = result['pred_labels']
            metrics = result['metrics']
            
            # === å·¦ä¾§ï¼šçœŸå®æ ‡ç­¾åˆ†å¸ƒ ===
            unique_true = np.unique(true_labels)
            cmap_true = plt.cm.get_cmap('tab10')
            
            for i, label in enumerate(unique_true):
                mask = (true_labels == label)
                class_name = self.class_names.get(label, f'Class {label}')
                color = cmap_true(i / max(len(unique_true), 10))  # å½’ä¸€åŒ–åˆ°[0,1]
                ax_true.scatter(features_2d[mask, 0], features_2d[mask, 1],
                              c=[color], label=class_name,
                              s=20, alpha=0.8, edgecolors='none', linewidth=0)
            
            ax_true.set_title(f'{method} (Ground Truth)', 
                            fontsize=10, fontweight='bold')
            ax_true.set_xlabel(' ', fontsize=9)
            ax_true.set_ylabel(' ', fontsize=9)
            # ax_true.legend(loc='best', fontsize=7, framealpha=0.8)
            ax_true.grid(True, alpha=0.2)
            
            # === å³ä¾§ï¼šèšç±»ç»“æœ ===
            unique_clusters = np.unique(pred_labels)
            cmap_cluster = plt.cm.get_cmap('Set3')
            
            for i, cluster in enumerate(unique_clusters):
                mask = (pred_labels == cluster)
                color = cmap_cluster(i / max(len(unique_clusters), 12))  # å½’ä¸€åŒ–åˆ°[0,1]
                ax_pred.scatter(features_2d[mask, 0], features_2d[mask, 1],
                              c=[color], label=f'C{cluster}',
                              s=20, alpha=0.8, edgecolors='none', linewidth=0)
            
            acc = metrics['accuracy_Nto1']
            ari = metrics['ari']
            nmi = metrics['nmi']
            sil = metrics['silhouette']
            # ax_pred.set_title(f'{method}\n(Clustering K={metrics["n_clusters"]})\n'
            #                 f'Acc: {acc:.3f} | ARI: {ari:.3f} | NMI: {nmi:.3f}',
            #                 fontsize=10, fontweight='bold')
            
            ax_pred.set_title(f'{method}',
                            fontsize=10, fontweight='bold')
            ax_pred.set_xlabel(' ', fontsize=9)
            ax_pred.set_ylabel(' ', fontsize=9)
            # ax_pred.legend(loc='best', fontsize=7, framealpha=0.8)
            ax_pred.grid(True, alpha=0.2)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_methods, n_rows * n_cols):
            row = idx // n_cols
            col_base = (idx % n_cols) * 2
            axes[row, col_base].axis('off')
            axes[row, col_base + 1].axis('off')
        
        # plt.suptitle('Clustering Results: Ground Truth vs Predictions',
        #             fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ä¿å­˜ PDF ç‰ˆæœ¬
        pdf_filename = f'benchmark_clustering_{self.config["dataset"]}.pdf'
        plt.savefig(pdf_filename, bbox_inches='tight')
        print(f"  âœ“ Saved: {pdf_filename}")
        
        if not self.config.get('no_show', False):
            plt.show()
        plt.close(fig)
    
    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_file = f"benchmark_results_{self.config['dataset']}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 140 + "\n")
            f.write(f"COMPREHENSIVE BENCHMARK RESULTS - {self.config['dataset']} Dataset\n")
            f.write("=" * 140 + "\n\n")
            
            # é…ç½®ä¿¡æ¯
            f.write("Configuration:\n")
            f.write("-" * 140 + "\n")
            for key, value in self.config.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # è¯¦ç»†ç»“æœ
            categories = {
                'Traditional Methods': ['Raw+UMAP', 'Handcrafted Features', 'PCA+K-Means'],
                'Deep Clustering': ['DEC', 'JULE', 'SCAN'],
                'Contrastive Learning (2020)': ['SimCLR', 'MoCo', 'BYOL'],
                'Contrastive Learning (SOTA 2021-2023)': ['SimSiam', 'TS2Vec', 'VICReg', 'TimesNet'],
                'TCCL (Ours)': ['TCCL']
            }
            
            for category, methods in categories.items():
                f.write(f"{category}:\n")
                f.write("-" * 140 + "\n")
                f.write(f"{'Method':<25} {'Accâ†‘':>8} {'ARIâ†‘':>8} {'NMIâ†‘':>8} {'Silâ†‘':>8} "
                       f"{'SepRâ†‘':>8} {'DBâ†“':>8} {'CHâ†‘':>9}\n")
                f.write("-" * 140 + "\n")
                
                for method in methods:
                    if method in self.results:
                        m = self.results[method]['metrics']
                        f.write(f"{method:<25} {m['accuracy_Nto1']:>8.4f} {m['ari']:>8.4f} "
                               f"{m['nmi']:>8.4f} {m['silhouette']:>8.4f} "
                               f"{m['separation_ratio']:>8.4f} {m['davies_bouldin']:>8.4f} "
                               f"{m['calinski_harabasz']:>9.1f}\n")
                f.write("\n")
            
            f.write("=" * 140 + "\n")
        
        print(f"\nâœ“ Results saved to: {output_file}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•æµç¨‹"""
        try:
            # æ­¥éª¤1: å‡†å¤‡æ•°æ®
            self.prepare_data()
            
            # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹
            self.create_models()
            
            # æ­¥éª¤3: è®­ç»ƒæ¨¡å‹
            self.train_all_models()
            
            # æ­¥éª¤4: è¯„ä¼°æ¨¡å‹
            self.evaluate_all_models()
            
            # æ­¥éª¤5: æ‰“å°å¯¹æ¯”è¡¨æ ¼
            self.print_comparison_table()
            
            # æ­¥éª¤6: å¯è§†åŒ–ç»“æœ
            self.visualize_results()
            
            # æ­¥éª¤7: ä¿å­˜ç»“æœ
            self.save_results()
            
            print("\n" + "=" * 80)
            print("âœ… BENCHMARK COMPLETED SUCCESSFULLY!")
            print("=" * 80)
            print(f"ğŸ“Š Total methods evaluated: {len(self.results)}")
            
            if 'TCCL' in self.results:
                tccl_metrics = self.results['TCCL']['metrics']
                print(f"\nğŸ† TCCL Performance:")
                print(f"   Accuracy: {tccl_metrics['accuracy_Nto1']:.4f}")
                print(f"   ARI: {tccl_metrics['ari']:.4f}")
                print(f"   NMI: {tccl_metrics['nmi']:.4f}")
                print(f"   Silhouette: {tccl_metrics['silhouette']:.4f}")
            
            print("=" * 80)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Benchmark interrupted by user")
        except Exception as e:
            print(f"\n\nâŒ Error during benchmark: {e}")
            import traceback
            traceback.print_exc()


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='Comprehensive Benchmark for TCCL',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', type=str, required=False,
                       choices=['CWRU', 'SEU', 'MFPT'],
                       help='Dataset name')
    parser.add_argument('--data_root', type=str, required=False,
                       help='Root directory of dataset')
    parser.add_argument('--all', action='store_true',
                       help='Run benchmark on all datasets (CWRU, SEU, MFPT) automatically')
    parser.add_argument('--cwru_root', type=str, default='E:/AI/CWRU-dataset-main/48k007',
                       help='CWRU dataset root (used with --all)')
    parser.add_argument('--seu_root', type=str, default='E:/AI/Mechanical-datasets-master/dataset',
                       help='SEU dataset root (used with --all)')
    parser.add_argument('--mfpt_root', type=str, default='E:/AI/MFPT-Fault-Data-Sets-20200227T131140Z-001/MFPT/MFPT',
                       help='MFPT dataset root (used with --all)')
    parser.add_argument('--window_size', type=int, default=1024,
                       help='Window size')
    parser.add_argument('--step_size', type=int, default=512,
                       help='Step size')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=5,
                       help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128,
                       help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--temperature', type=float, default=0.01,
                       help='Temperature for contrastive loss')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed for reproducibility')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default=None,
                       help='Device (cuda/cpu)')
    parser.add_argument('--num_workers', type=int, default=0,
                       help='Number of data loading workers')
    parser.add_argument('--no_umap', action='store_true',
                       help='Disable UMAP for clustering')
    parser.add_argument('--no_visualize_clusters', action='store_true',
                       help='Disable cluster visualization')
    parser.add_argument('--no_show', action='store_true',
                       help='Do not show plots (only save to files)')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.all and (not args.dataset or not args.data_root):
        parser.error('--dataset and --data_root are required unless --all is specified')
    
    # è½¬æ¢ä¸ºé…ç½®å­—å…¸
    config = {
        'dataset': args.dataset,
        'data_root': args.data_root,
        'run_all': args.all,
        'no_show': args.no_show or args.all,  # --all æ¨¡å¼è‡ªåŠ¨å¯ç”¨ no_show
        'window_size': args.window_size,
        'step_size': args.step_size,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'temperature': args.temperature,
        'device': args.device,
        'num_workers': args.num_workers,
        'use_umap': not args.no_umap,
        'visualize_clusters': not args.no_visualize_clusters,
        'noise_level': 0.05,
        'scale_range': (0.9, 1.1),
        'seed': args.seed,
    }
    
    return config, args


# ç»Ÿä¸€éšæœºç§å­è®¾ç½®ï¼ˆä¸ single_tccl.py ä¸€è‡´ï¼‰
def set_random_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ“ éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


# æŒ‰ single_tccl.py çš„æ–¹æ¡ˆè¯„ä¼°èšç±»ï¼ˆä¸åšL2ï¼›UMAP->2Dï¼›DBSCANï¼›å™ªå£°å›å¡«ï¼‰
def evaluate_clustering_single_style(features: np.ndarray, labels: np.ndarray) -> Dict:
    n_true_classes = len(np.unique(labels))
    print(f"  UMAP dimensionality reduction to 2D (True classes={n_true_classes})...")
    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    features_2d = reducer.fit_transform(features)

    print(f"  Estimating eps parameter...")
    n_samples = len(features_2d)
    min_samples = max(10, min(n_samples // 100, n_true_classes * 5))

    nbrs = NearestNeighbors(n_neighbors=min_samples).fit(features_2d)
    distances, _ = nbrs.kneighbors(features_2d)
    k_distances = np.sort(distances[:, -1])

    eps_candidates = [
        np.percentile(k_distances, 95),
        np.percentile(k_distances, 96),
        np.percentile(k_distances, 97),
        np.percentile(k_distances, 98),
    ]
    eps = np.median(eps_candidates)

    print(f"  Running DBSCAN on 2D space (eps={eps:.4f}, min_samples={min_samples})...")
    max_attempts = 3
    for attempt in range(max_attempts):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
        pred_labels = dbscan.fit_predict(features_2d)
        n_clusters = len(set(pred_labels)) - (1 if -1 in pred_labels else 0)
        n_noise = list(pred_labels).count(-1)
        print(f"  â†’ Attempt {attempt+1}: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(pred_labels)*100:.1f}%)")
        if n_clusters <= n_true_classes * 3:
            print(f"  âœ“ Cluster count is reasonable (â‰¤ {n_true_classes * 3})")
            break
        elif attempt < max_attempts - 1:
            eps *= 1.3
            print(f"  âš  Too many clusters, increasing eps to {eps:.4f}...")
        else:
            print(f"  âš  Warning: Still {n_clusters} clusters after {max_attempts} attempts")

    if n_noise > 0:
        valid_mask = pred_labels != -1
        valid_features = features_2d[valid_mask]
        valid_labels = pred_labels[valid_mask]
        noise_mask = pred_labels == -1
        noise_features = features_2d[noise_mask]
        if len(valid_features) > 0:
            knn = KNeighborsClassifier(n_neighbors=min(5, len(valid_features)))
            knn.fit(valid_features, valid_labels)
            pred_labels[noise_mask] = knn.predict(noise_features)
            print(f"  Reassigned noise points to nearest clusters")

    unique_labels = np.unique(pred_labels)
    label_mapping = {old: new for new, old in enumerate(unique_labels)}
    pred_labels = np.array([label_mapping[lbl] for lbl in pred_labels])
    n_clusters = len(unique_labels)

    ari = adjusted_rand_score(labels, pred_labels)
    nmi = normalized_mutual_info_score(labels, pred_labels)
    if n_clusters > 1:
        sil = silhouette_score(features_2d, pred_labels)
    else:
        sil = 0.0

    acc = compute_cluster_accuracy(labels, pred_labels)

    return {
        'acc': acc,
        'ari': ari,
        'nmi': nmi,
        'sil': sil,
        'pred_labels': pred_labels,
        'n_clusters': n_clusters,
        'n_true_classes': n_true_classes,
        'features_reduced': features_2d,
    }


def compute_cluster_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    from sklearn.metrics import confusion_matrix
    y_true = y_true.astype(np.int64)
    y_pred = y_pred.astype(np.int64)
    cm = confusion_matrix(y_true, y_pred)
    n_clusters = len(np.unique(y_pred))
    best_mapping = {}
    for cluster_id in range(n_clusters):
        best_class = np.argmax(cm[:, cluster_id])
        best_mapping[cluster_id] = best_class
    mapped_pred = np.array([best_mapping[p] for p in y_pred])
    accuracy = np.mean(mapped_pred == y_true)
    return accuracy


def main():
    """ä¸»å‡½æ•°"""
    config, args = parse_args()
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ --all æ¨¡å¼
    if args.all:
        print("=" * 80)
        print("ğŸš€ RUNNING BENCHMARK ON ALL DATASETS")
        print("=" * 80)
        
        # å®šä¹‰æ‰€æœ‰æ•°æ®é›†é…ç½®ï¼ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é»˜è®¤å€¼ï¼‰
        datasets_config = {
            'CWRU': args.cwru_root,
            'SEU': args.seu_root,
            'MFPT': args.mfpt_root
        }
        
        for dataset_name, data_root in datasets_config.items():
            print(f"\n{'='*80}")
            print(f"ğŸ“Š Processing dataset: {dataset_name}")
            print(f"{'='*80}")
            
            # åˆ›å»ºå½“å‰æ•°æ®é›†çš„é…ç½®
            current_config = config.copy()
            current_config['dataset'] = dataset_name
            current_config['data_root'] = data_root
            
            try:
                runner = BenchmarkRunner(current_config)
                runner.run()
                print(f"\nâœ… {dataset_name} completed successfully!")
            except Exception as e:
                print(f"\nâŒ Error processing {dataset_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print("\n" + "=" * 80)
        print("âœ… ALL DATASETS COMPLETED!")
        print("=" * 80)
    else:
        # å•æ•°æ®é›†æ¨¡å¼
        runner = BenchmarkRunner(config)
        runner.run()


if __name__ == '__main__':
    main()

