"""
Single-File TCCL Implementation
å®Œæ•´çš„TCCLå®ç°ï¼ŒåŒ…å«æ•°æ®é›†åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå’Œå¯è§†åŒ–
ä¸ä¾èµ–å…¶ä»–é¡¹ç›®æ–‡ä»¶ï¼Œå¯ç‹¬ç«‹è¿è¡Œ

Usage:
    # ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œä¸‰ä¸ªæ•°æ®é›†
    python single_tccl.py
    
    # è‡ªå®šä¹‰éšæœºç§å­å’Œè®­ç»ƒå‚æ•°
    python single_tccl.py --seed 123 --epochs 100 --batch_size 64 --lr 0.0001
    
    # è‡ªå®šä¹‰æ•°æ®é›†è·¯å¾„
    python single_tccl.py --cwru_path /path/to/cwru --seu_path /path/to/seu --mfpt_path /path/to/mfpt
    
    # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°
    python single_tccl.py --help
"""

import os
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score, confusion_matrix
from sklearn.manifold import TSNE
import umap
from scipy.io import loadmat
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# éšæœºç§å­è®¾ç½®
# ============================================================================

def set_random_seed(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
    
    Args:
        seed: éšæœºç§å­å€¼ï¼Œé»˜è®¤ä¸º42
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿å®Œå…¨çš„ç¡®å®šæ€§
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"âœ“ éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


# ============================================================================
# æ•°æ®é›†åŠ è½½ç±»
# ============================================================================

class CWRUDataset(Dataset):
    """CWRUè½´æ‰¿æ•°æ®é›†"""
    
    def __init__(self, data_root, window_size=1024, stride=512, split='full', augment=False, verbose=True):
        """
        Args:
            data_root: æ•°æ®æ ¹ç›®å½•
            window_size: çª—å£å¤§å°
            stride: æ»‘åŠ¨æ­¥é•¿
            split: 'train', 'test', 'full'
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†åŠ è½½ä¿¡æ¯
        """
        self.window_size = window_size
        self.stride = stride
        self.augment = augment
        self.verbose = verbose
        
        # åŠ è½½æ•°æ®
        self.data, self.labels = self._load_data(data_root)
        
        # å½’ä¸€åŒ–
        self.mean = np.mean(self.data)
        self.std = np.std(self.data)
        self.data = (self.data - self.mean) / (self.std + 1e-8)
        
        if verbose:
            print(f"[CWRU] åŠ è½½å®Œæˆï¼å…± {len(self.data)} ä¸ªæ ·æœ¬")
            print(f"[CWRU] å„ç±»åˆ«æ ·æœ¬æ•°: {self._count_labels()}")
    
    def _load_data(self, data_root):
        """åŠ è½½CWRUæ•°æ®é›†"""
        data_list = []
        label_list = []
        
        label_map = {
            'normal': 0,
            'ball': 1,
            'inner_race': 2,
            'outer_race': 3
        }
        
        # æ‰«ææ‰€æœ‰.matæ–‡ä»¶
        mat_files = [f for f in os.listdir(data_root) if f.endswith('.mat')]
        
        if self.verbose:
            print(f"[CWRU] å‘ç° {len(mat_files)} ä¸ª.matæ–‡ä»¶")
        
        for mat_file in sorted(mat_files):
            file_path = os.path.join(data_root, mat_file)
            
            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­ç±»åˆ« - CWRUæ•°æ®é›†çš„å‘½åè§„åˆ™
            # æ­£å¸¸: 97.mat, 98.mat, 99.mat, 100.mat ç­‰ï¼ˆçº¯æ•°å­—ï¼‰
            # æ»šç æ•…éšœ: B007_0.mat, B014_1.mat, B021_2.mat ç­‰ï¼ˆä»¥Bå¼€å¤´ï¼‰
            # å†…åœˆæ•…éšœ: IR007_0.mat, IR014_1.mat, IR021_2.mat ç­‰ï¼ˆä»¥IRå¼€å¤´ï¼‰
            # å¤–åœˆæ•…éšœ: OR007@6_0.mat, OR014@6_1.mat ç­‰ï¼ˆä»¥ORå¼€å¤´ï¼‰
            
            fname_upper = mat_file.upper()
            
            if fname_upper.startswith('IR'):
                label = label_map['inner_race']
            elif fname_upper.startswith('OR'):
                label = label_map['outer_race']
            elif fname_upper.startswith('B') and not fname_upper.startswith('BA'):
                # Bå¼€å¤´ä½†ä¸æ˜¯BAï¼ˆBAæ˜¯baseline accelerometerçš„ç¼©å†™ï¼‰
                label = label_map['ball']
            elif any(x in fname_upper for x in ['NORMAL', 'BASELINE']) or mat_file[0].isdigit():
                # åŒ…å«normal/baselineå…³é”®å­—ï¼Œæˆ–è€…æ–‡ä»¶åä»¥æ•°å­—å¼€å¤´
                label = label_map['normal']
            else:
                print(f"  è·³è¿‡æ— æ³•è¯†åˆ«çš„æ–‡ä»¶: {mat_file}")
                continue
            
            # åŠ è½½.matæ–‡ä»¶
            try:
                mat_data = loadmat(file_path)
                # CWRUæ•°æ®é›†ä¸­ï¼ŒæŒ¯åŠ¨ä¿¡å·é€šå¸¸åœ¨DE_time, FE_timeç­‰é”®ä¸­
                signal_loaded = False
                for key in mat_data.keys():
                    if 'DE_time' in key or 'FE_time' in key or 'BA_time' in key:
                        signal = mat_data[key].flatten()
                        
                        # æ»‘åŠ¨çª—å£åˆ‡ç‰‡
                        num_windows = 0
                        for i in range(0, len(signal) - self.window_size, self.stride):
                            window = signal[i:i + self.window_size]
                            data_list.append(window)
                            label_list.append(label)
                            num_windows += 1
                        
                        label_name = [k for k, v in label_map.items() if v == label][0]
                        if self.verbose:
                            print(f"  âœ“ {mat_file:30s} -> {label_name:12s} ({num_windows:4d} windows)")
                        signal_loaded = True
                        break
                
                if not signal_loaded and self.verbose:
                    print(f"  âœ— {mat_file}: æœªæ‰¾åˆ°æŒ¯åŠ¨ä¿¡å·é”®")
                    
            except Exception as e:
                if self.verbose:
                    print(f"  âœ— {mat_file}: åŠ è½½å¤±è´¥ - {e}")
                continue
        
        return np.array(data_list, dtype=np.float32), np.array(label_list, dtype=np.int64)
    
    def _count_labels(self):
        """ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°"""
        unique, counts = np.unique(self.labels, return_counts=True)
        label_names = {0: 'normal', 1: 'ball', 2: 'inner_race', 3: 'outer_race'}
        return {label_names[k]: v for k, v in zip(unique, counts)}
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        label = self.labels[idx]
        
        if self.augment:
            # æ•°æ®å¢å¼º
            x1 = self._augment(x)
            x2 = self._augment(x)
            return torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x2).unsqueeze(0), label
        else:
            return torch.FloatTensor(x).unsqueeze(0), label
    
    def _augment(self, x):
        """æ•°æ®å¢å¼º"""
        # å¹…å€¼ç¼©æ”¾
        scale = np.random.uniform(0.9, 1.1)
        x = x * scale
        
        # é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, 0.05, x.shape)
        x = x + noise
        
        return x


class SEUDataset(Dataset):
    """SEUè½´æ‰¿æ•°æ®é›†"""
    
    def __init__(self, data_root, window_size=1024, stride=512, split='full', augment=False, verbose=True):
        self.window_size = window_size
        self.stride = stride
        self.augment = augment
        self.verbose = verbose
        
        self.data, self.labels = self._load_data(data_root)
        
        # å½’ä¸€åŒ–
        self.mean = np.mean(self.data)
        self.std = np.std(self.data)
        self.data = (self.data - self.mean) / (self.std + 1e-8)
        
        if verbose:
            print(f"[SEU] åŠ è½½å®Œæˆï¼å…± {len(self.data)} ä¸ªæ ·æœ¬")
            print(f"[SEU] å„ç±»åˆ«æ ·æœ¬æ•°: {self._count_labels()}")
    
    def _load_data(self, data_root):
        """åŠ è½½SEUæ•°æ®é›†"""
        data_list = []
        label_list = []
        
        label_map = {
            'normal': 0,
            'ball': 1,
            'inner_race': 2,
            'outer_race': 3
        }
        
        mat_files = [f for f in os.listdir(data_root) if f.endswith('.mat')]
        if self.verbose:
            print(f"\n[SEU] æ‰¾åˆ° {len(mat_files)} ä¸ª.matæ–‡ä»¶")
        
        for mat_file in sorted(mat_files):
            file_path = os.path.join(data_root, mat_file)
            
            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­ç±»åˆ« - SEUæ•°æ®é›†çš„å‘½åè§„åˆ™
            # SEUä½¿ç”¨ç®€å†™ï¼šnormal, ir (å†…åœˆ), or (å¤–åœˆ), b (æ»šç )
            # æ–‡ä»¶åç¤ºä¾‹ï¼šnormal_0.mat, ir007.mat, or007@6.mat, b007.mat
            fname_lower = mat_file.lower()
            fname_base = mat_file.replace('.mat', '').lower()
            
            # ä¼˜å…ˆçº§åˆ¤æ–­ï¼ˆå…³é”®è¯åŒ¹é…é¡ºåºå¾ˆé‡è¦ï¼‰
            if 'normal' in fname_lower:
                label = label_map['normal']
                label_name = 'normal'
            elif fname_base.startswith('ir') or '_ir' in fname_lower:
                # IRå¼€å¤´æˆ–åŒ…å«_irçš„æ˜¯å†…åœˆæ•…éšœ
                label = label_map['inner_race']
                label_name = 'inner_race'
            elif fname_base.startswith('or') or '_or' in fname_lower:
                # ORå¼€å¤´æˆ–åŒ…å«_orçš„æ˜¯å¤–åœˆæ•…éšœ
                label = label_map['outer_race']
                label_name = 'outer_race'
            elif fname_base.startswith('b') or '_b' in fname_lower or 'ball' in fname_lower:
                # Bå¼€å¤´æˆ–åŒ…å«_bæˆ–ballçš„æ˜¯æ»šç æ•…éšœ
                label = label_map['ball']
                label_name = 'ball'
            else:
                if self.verbose:
                    print(f"  [SEU] è·³è¿‡æœªè¯†åˆ«çš„æ–‡ä»¶: {mat_file}")
                continue
            
            if self.verbose:
                print(f"  [SEU] {mat_file} -> {label_name}")
            
            try:
                mat_data = loadmat(file_path)
                # SEUæ•°æ®é›†çš„ä¿¡å·å­˜å‚¨åœ¨ä»¥ _DE_time æˆ– _FE_time ç»“å°¾çš„é”®ä¸­
                signal = None
                
                # ä¼˜å…ˆæŸ¥æ‰¾é©±åŠ¨ç«¯ä¿¡å· (DE = Drive End)
                for key in mat_data.keys():
                    if key.endswith('_DE_time'):
                        signal = mat_data[key].flatten()
                        break
                
                # å¦‚æœæ‰¾ä¸åˆ°ï¼ŒæŸ¥æ‰¾é£æ‰‡ç«¯ä¿¡å· (FE = Fan End)
                if signal is None:
                    for key in mat_data.keys():
                        if key.endswith('_FE_time'):
                            signal = mat_data[key].flatten()
                            break
                
                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾ä»»ä½•éå…ƒæ•°æ®é”®
                if signal is None:
                    for key in mat_data.keys():
                        if not key.startswith('__'):
                            signal = mat_data[key].flatten()
                            break
                
                if signal is not None and len(signal) > self.window_size:
                    n_windows_before = len(data_list)
                    for i in range(0, len(signal) - self.window_size, self.stride):
                        window = signal[i:i + self.window_size]
                        data_list.append(window)
                        label_list.append(label)
                    if self.verbose:
                        n_windows_added = len(data_list) - n_windows_before
                        print(f"    -> ç”Ÿæˆ {n_windows_added} ä¸ªçª—å£")
                else:
                    if self.verbose:
                        print(f"    -> ä¿¡å·é•¿åº¦ä¸è¶³ ({len(signal) if signal is not None else 0})")
            except Exception as e:
                if self.verbose:
                    print(f"  âš ï¸  åŠ è½½å¤±è´¥: {mat_file} - {e}")
                continue
        
        return np.array(data_list, dtype=np.float32), np.array(label_list, dtype=np.int64)
    
    def _count_labels(self):
        unique, counts = np.unique(self.labels, return_counts=True)
        label_names = {0: 'normal', 1: 'ball', 2: 'inner_race', 3: 'outer_race'}
        return {label_names[k]: v for k, v in zip(unique, counts)}
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        label = self.labels[idx]
        
        if self.augment:
            x1 = self._augment(x)
            x2 = self._augment(x)
            return torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x2).unsqueeze(0), label
        else:
            return torch.FloatTensor(x).unsqueeze(0), label
    
    def _augment(self, x):
        scale = np.random.uniform(0.9, 1.1)
        x = x * scale
        noise = np.random.normal(0, 0.05, x.shape)
        x = x + noise
        return x


class MFPTDataset(Dataset):
    """MFPTè½´æ‰¿æ•°æ®é›†"""
    
    def __init__(self, data_root, window_size=1024, stride=512, split='full', augment=False, verbose=True):
        self.window_size = window_size
        self.stride = stride
        self.augment = augment
        self.verbose = verbose
        
        self.data, self.labels = self._load_data(data_root)
        
        # å½’ä¸€åŒ–
        self.mean = np.mean(self.data)
        self.std = np.std(self.data)
        self.data = (self.data - self.mean) / (self.std + 1e-8)
        
        if verbose:
            print(f"[MFPT] åŠ è½½å®Œæˆï¼å…± {len(self.data)} ä¸ªæ ·æœ¬")
            print(f"[MFPT] å„ç±»åˆ«æ ·æœ¬æ•°: {self._count_labels()}")
    
    def _load_data(self, data_root):
        """åŠ è½½MFPTæ•°æ®é›†"""
        data_list = []
        label_list = []
        
        label_map = {
            'normal': 0,
            'inner_race': 1,
            'outer_race': 2
        }
        
        mat_files = [f for f in os.listdir(data_root) if f.endswith('.mat')]
        if self.verbose:
            print(f"\n[MFPT] æ‰¾åˆ° {len(mat_files)} ä¸ª.matæ–‡ä»¶")
        
        for mat_file in sorted(mat_files):
            file_path = os.path.join(data_root, mat_file)
            
            # MFPTæ•°æ®é›†æ–‡ä»¶åè§„åˆ™ï¼š
            # baseline*.mat â†’ normal
            # *innerracefault*.mat æˆ– *inner*.mat â†’ inner_race
            # *outerracefault*.mat æˆ– *outer*.mat â†’ outer_race
            fname_lower = mat_file.lower()
            
            if 'baseline' in fname_lower:
                label = label_map['normal']
                label_name = 'normal'
            elif 'innerracefault' in fname_lower or ('inner' in fname_lower and 'race' in fname_lower):
                label = label_map['inner_race']
                label_name = 'inner_race'
            elif 'outerracefault' in fname_lower or ('outer' in fname_lower and 'race' in fname_lower):
                label = label_map['outer_race']
                label_name = 'outer_race'
            elif 'inner' in fname_lower:
                label = label_map['inner_race']
                label_name = 'inner_race'
            elif 'outer' in fname_lower:
                label = label_map['outer_race']
                label_name = 'outer_race'
            else:
                if self.verbose:
                    print(f"  [MFPT] è·³è¿‡æœªè¯†åˆ«çš„æ–‡ä»¶: {mat_file}")
                continue
            
            if self.verbose:
                print(f"  [MFPT] {mat_file} -> {label_name}")
            
            try:
                mat_data = loadmat(file_path)
                signal = None
                
                # MFPTæ•°æ®é›†ç‰¹å®šçš„æ•°æ®ç»“æ„: mat_data['bearing']['gs'][0, 0]
                try:
                    signal = mat_data['bearing']['gs'][0, 0].flatten()
                except:
                    # å¦‚æœæ ‡å‡†ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–é”®
                    for key in ['bearing', 'gs', 'sr']:
                        if key in mat_data:
                            if isinstance(mat_data[key], np.ndarray):
                                if mat_data[key].dtype.names:  # ç»“æ„ä½“
                                    for field in mat_data[key].dtype.names:
                                        if 'gs' in field or 'sr' in field or 'vibration' in field:
                                            signal = mat_data[key][field][0][0].flatten()
                                            break
                                else:
                                    signal = mat_data[key].flatten()
                                break
                    
                    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                    if signal is None:
                        for key in mat_data.keys():
                            if not key.startswith('__'):
                                try:
                                    signal = mat_data[key].flatten()
                                    break
                                except:
                                    continue
                
                if signal is not None and len(signal) > self.window_size:
                    n_windows_before = len(data_list)
                    for i in range(0, len(signal) - self.window_size, self.stride):
                        window = signal[i:i + self.window_size]
                        data_list.append(window)
                        label_list.append(label)
                    if self.verbose:
                        n_windows_added = len(data_list) - n_windows_before
                        print(f"    -> ç”Ÿæˆ {n_windows_added} ä¸ªçª—å£")
                else:
                    if self.verbose:
                        print(f"    -> ä¿¡å·é•¿åº¦ä¸è¶³ ({len(signal) if signal is not None else 0})")
            except Exception as e:
                if self.verbose:
                    print(f"  âš ï¸  åŠ è½½å¤±è´¥: {mat_file} - {e}")
                continue
        
        return np.array(data_list, dtype=np.float32), np.array(label_list, dtype=np.int64)
    
    def _count_labels(self):
        unique, counts = np.unique(self.labels, return_counts=True)
        label_names = {0: 'normal', 1: 'inner_race', 2: 'outer_race'}
        return {label_names[k]: v for k, v in zip(unique, counts)}
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        label = self.labels[idx]
        
        if self.augment:
            x1 = self._augment(x)
            x2 = self._augment(x)
            return torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x2).unsqueeze(0), label
        else:
            return torch.FloatTensor(x).unsqueeze(0), label
    
    def _augment(self, x):
        scale = np.random.uniform(0.9, 1.1)
        x = x * scale
        noise = np.random.normal(0, 0.05, x.shape)
        x = x + noise
        return x


# ============================================================================
# TCCLæ¨¡å‹å®šä¹‰
# ============================================================================

class FeatureEncoder(nn.Module):
    """ç‰¹å¾ç¼–ç å™¨ - 3å±‚1D-CNN"""
    
    def __init__(self, in_channels=1, feature_dim=64):
        super().__init__()
        
        self.encoder = nn.Sequential(
            # Block 1: 1 -> 32, 1024 -> 256
            nn.Conv1d(in_channels, 16, kernel_size=17, stride=1, padding=8),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4, stride=4),
            
            # Block 2: 32 -> 64, 256 -> 64
            nn.Conv1d(16, 32, kernel_size=17, stride=1, padding=8),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4, stride=4),
            
            # Block 3: 64 -> 128, 64 -> 16
            nn.Conv1d(32, feature_dim, kernel_size=17, stride=1, padding=8),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4, stride=4),
        )
    
    def forward(self, x):
        """
        Args:
            x: (B, 1, L) - è¾“å…¥ä¿¡å·
        Returns:
            h: (B, C, W) - ç‰¹å¾å›¾ï¼ŒC=64, W=16
        """
        return self.encoder(x)

class TemplateHead(nn.Module):
    """æ¨¡æ¿å¤´ - ç”Ÿæˆæ¨¡æ¿æ ¸ï¼ˆä¸benchmark.pyå®Œå…¨ä¸€è‡´ï¼‰"""
    
    def __init__(self, in_channels=64, kernel_width=5):
        super().__init__()
        self.kernel_width = kernel_width
        
        # ä¸benchmarkä¸­TCCLModelçš„template_headå®Œå…¨ä¸€è‡´
        self.head = nn.Sequential(
            nn.Conv1d(in_channels, in_channels, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm1d(in_channels),
            nn.ReLU(),
            nn.Conv1d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(in_channels),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(kernel_width)
        )
    
    def forward(self, h):
        """
        Args:
            h: (B, C, W) - ç‰¹å¾å›¾
        Returns:
            k: (B, C, W_k) - æ¨¡æ¿æ ¸ï¼ŒW_k=8
        """
        return self.head(h)


class TCCLModel(nn.Module):
    """å®Œæ•´çš„TCCLæ¨¡å‹ï¼ˆä¸benchmark.pyå®Œå…¨ä¸€è‡´ï¼‰"""
    
    def __init__(self, feature_dim=64, kernel_width=5, temperature=0.1, template_lambda=0.5):
        super().__init__()
        
        self.encoder = FeatureEncoder(in_channels=1, feature_dim=feature_dim)
        self.template_head = TemplateHead(in_channels=feature_dim, kernel_width=kernel_width)
        self.temperature = temperature
        self.template_lambda = template_lambda  # æ¨¡æ¿å¯¹æ¯”æŸå¤±æƒé‡
    
    def forward(self, x1, x2):
        """
        Args:
            x1, x2: (B, 1, L) - ä¸¤ä¸ªå¢å¼ºè§†å›¾
        Returns:
            loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        # æå–ç‰¹å¾
        h1 = self.encoder(x1)  # (B, C, W)
        h2 = self.encoder(x2)  # (B, C, W)
        
        # ç”Ÿæˆæ¨¡æ¿æ ¸
        k1 = self.template_head(h1)  # (B, C, W_k)
        k2 = self.template_head(h2)  # (B, C, W_k)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_1to2 = self._template_similarity(k1, h2)  # (B, B)
        sim_2to1 = self._template_similarity(k2, h1)  # (B, B)
        
        # å¯¹ç§°çš„InfoNCEæŸå¤±ï¼ˆç‰¹å¾-æ¨¡æ¿åŒ¹é…æŸå¤±ï¼‰
        loss_1to2 = self._infonce_loss(sim_1to2)
        loss_2to1 = self._infonce_loss(sim_2to1)
        loss_matching = (loss_1to2 + loss_2to1) / 2
        
        # æ¨¡æ¿å¯¹æ¯”æŸå¤±ï¼ˆå¢å¼ºæ¨¡æ¿åˆ¤åˆ«æ€§ï¼‰
        loss_template = self._template_contrastive_loss(k1, k2)
        
        # æ€»æŸå¤±
        loss = loss_matching + self.template_lambda * loss_template
        
        return loss
    
    def _template_similarity(self, k, h):
        """
        è®¡ç®—æ¨¡æ¿æ ¸ä¸ç‰¹å¾å›¾çš„æ»‘åŠ¨ä½™å¼¦ç›¸ä¼¼åº¦ (Sliding Cosine Similarity)
        é€šè¿‡åœ¨å·ç§¯å‰å¯¹æ¨¡æ¿æ ¸å’Œç‰¹å¾å›¾è¿›è¡ŒL2å½’ä¸€åŒ–ï¼Œå°†æ ‡å‡†äº’ç›¸å…³è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            k: (B, C, W_k) - æ¨¡æ¿æ ¸
            h: (B, C, W) - ç‰¹å¾å›¾
        Returns:
            sim: (B, B) - ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        B, C, W_k = k.shape
        _, _, W = h.shape
        
        # æ‰©å±•æ¨¡æ¿å’Œç‰¹å¾å›¾ä»¥è®¡ç®—æ‰€æœ‰é…å¯¹
        templates_exp = k.unsqueeze(1).expand(-1, B, -1, -1)  # (B, B, C, W_k)
        templates_exp = templates_exp.reshape(B * B, C, W_k)
        
        features_exp = h.unsqueeze(0).expand(B, -1, -1, -1)  # (B, B, C, W)
        features_exp = features_exp.reshape(B * B, C, W)
        
        # é‡å¡‘ä¸ºåˆ†ç»„å·ç§¯æ ¼å¼
        templates_conv = templates_exp.reshape(B * B * C, 1, W_k)
        features_conv = features_exp.reshape(1, B * B * C, W)
        
        # é›¶ä¸­å¿ƒåŒ– + L2å½’ä¸€åŒ–ï¼šå°†æ ‡å‡†äº’ç›¸å…³è½¬æ¢ä¸ºæ»‘åŠ¨ä½™å¼¦ç›¸ä¼¼åº¦
        # å…ˆé›¶ä¸­å¿ƒåŒ–ï¼ˆå‡å»å‡å€¼ï¼‰ï¼Œè®©æ•°æ®åŒ…å«æ­£è´Ÿå€¼
        templates_conv = templates_conv - templates_conv.mean(dim=-1, keepdim=True)
        features_conv = features_conv - features_conv.mean(dim=-1, keepdim=True)
        
        # å†è¿›è¡ŒL2å½’ä¸€åŒ–
        # å¯¹æ¨¡æ¿æ ¸è¿›è¡ŒL2å½’ä¸€åŒ– (åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼Œå³ W_k ç»´åº¦)
        templates_conv = F.normalize(templates_conv, p=2, dim=-1)
        # å¯¹ç‰¹å¾å›¾è¿›è¡ŒL2å½’ä¸€åŒ– (åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼Œå³ W ç»´åº¦)
        features_conv = F.normalize(features_conv, p=2, dim=-1)
        
        # åˆ†ç»„å·ç§¯ï¼šç°åœ¨è®¡ç®—çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦è€Œéäº’ç›¸å…³
        # Output = Î£(h/|h| Â· k/|k|)ï¼Œç­‰ä»·äº cos(Î¸)
        conv_out = F.conv1d(features_conv, templates_conv, groups=B * B * C, padding='same')
        conv_out = conv_out.view(B * B, C, W)
        
        # æ²¿é€šé“å’Œæ—¶é—´ç»´åº¦èšåˆ
        response = torch.sum(conv_out, dim=1)  # (B*B, W)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        similarity = F.adaptive_avg_pool1d(response.unsqueeze(1), 1).squeeze()
        
        return similarity.view(B, B)
    
    def _infonce_loss(self, sim_matrix):
        """
        InfoNCEæŸå¤±ï¼ˆNT-Xentï¼‰- ä½¿ç”¨æ•°å€¼ç¨³å®šçš„å®ç°
        Args:
            sim_matrix: (B, B) - ç›¸ä¼¼åº¦çŸ©é˜µ
        Returns:
            loss: æ ‡é‡æŸå¤±
        """
        B = sim_matrix.shape[0]
        
        # æ£€æŸ¥ç›¸ä¼¼åº¦çŸ©é˜µæ˜¯å¦åŒ…å«å¼‚å¸¸å€¼
        if torch.isnan(sim_matrix).any() or torch.isinf(sim_matrix).any():
            print(f"è­¦å‘Š: ç›¸ä¼¼åº¦çŸ©é˜µåŒ…å«å¼‚å¸¸å€¼! nan: {torch.isnan(sim_matrix).sum()}, inf: {torch.isinf(sim_matrix).sum()}")
            # æ›¿æ¢å¼‚å¸¸å€¼ä¸º0
            sim_matrix = torch.nan_to_num(sim_matrix, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # ç¼©æ”¾ç›¸ä¼¼åº¦ï¼ˆæ¸©åº¦å‚æ•°ï¼‰
        logits = sim_matrix / self.temperature
        
        # æ­£æ ·æœ¬æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ï¼‰
        labels = torch.arange(B, device=sim_matrix.device)
        
        # ä½¿ç”¨PyTorchçš„æ•°å€¼ç¨³å®šcross_entropy
        # cross_entropyå†…éƒ¨ä½¿ç”¨log_softmaxï¼Œé¿å…äº†expæº¢å‡º
        loss = F.cross_entropy(logits, labels)
        
        return loss
    
    def _template_contrastive_loss(self, k1, k2):
        """
        æ¨¡æ¿å¯¹æ¯”æŸå¤±ï¼šè®©æ­£æ ·æœ¬å¯¹çš„æ¨¡æ¿ç›¸ä¼¼ï¼Œè´Ÿæ ·æœ¬å¯¹çš„æ¨¡æ¿è¿œç¦»
        é€šè¿‡åœ¨æ¨¡æ¿ç©ºé—´æ–½åŠ å¯¹æ¯”çº¦æŸï¼Œå¢å¼ºæ¨¡æ¿çš„åˆ¤åˆ«æ€§
        
        Args:
            k1, k2: (B, C, W_k) - åŒä¸€æ ·æœ¬ä¸¤ä¸ªå¢å¼ºè§†å›¾çš„æ¨¡æ¿æ ¸
        Returns:
            loss: æ ‡é‡æŸå¤±
        """
        B, C, W_k = k1.shape
        
        # å±•å¹³æ¨¡æ¿æ ¸ï¼š(B, C*W_k)
        k1_flat = k1.view(B, -1)
        k2_flat = k2.view(B, -1)
        
        # L2å½’ä¸€åŒ–
        k1_norm = F.normalize(k1_flat, p=2, dim=1)
        k2_norm = F.normalize(k2_flat, p=2, dim=1)
        
        # è®¡ç®—æ¨¡æ¿ç›¸ä¼¼åº¦çŸ©é˜µï¼š(B, B)
        # k1_norm @ k2_norm.T = cosine similarity
        template_sim = torch.matmul(k1_norm, k2_norm.T)
        
        # ä½¿ç”¨InfoNCEæŸå¤±ï¼šå¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬å¯¹ï¼ˆåŒä¸€æ ·æœ¬çš„ä¸¤ä¸ªè§†å›¾ï¼‰
        # éå¯¹è§’çº¿æ˜¯è´Ÿæ ·æœ¬å¯¹ï¼ˆä¸åŒæ ·æœ¬ï¼‰
        logits = template_sim / self.temperature
        labels = torch.arange(B, device=k1.device)
        
        loss = F.cross_entropy(logits, labels)
        
        return loss
    
    def extract_features(self, x):
        """æå–ç‰¹å¾ç”¨äºè¯„ä¼°"""
        h = self.encoder(x)  # (B, C, W)
        # å…¨å±€å¹³å‡æ± åŒ–
        # z = h.mean(dim=2)  # (B, C)
        # å±•å¹³æ“ä½œ
        z = h.view(h.size(0), -1)  # (B, C*W)
        # å½’ä¸€åŒ–æ“ä½œ
        # z = z / torch.norm(z, dim=1, keepdim=True)
        return z


# ============================================================================
# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
# ============================================================================

def train_tccl(model, train_loader, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for x1, x2, _ in pbar:
        x1, x2 = x1.to(device), x2.to(device)
        
        optimizer.zero_grad()
        loss = model(x1, x2)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / len(train_loader)


def extract_all_features(model, data_loader, device):
    """æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾"""
    model.eval()
    features_list = []
    labels_list = []
    
    with torch.no_grad():
        for x, labels in tqdm(data_loader, desc='Extracting features'):
            x = x.to(device)
            features = model.extract_features(x)
            features_list.append(features.cpu().numpy())
            labels_list.append(labels.numpy())
    
    features = np.concatenate(features_list, axis=0)
    labels = np.concatenate(labels_list, axis=0)
    
    return features, labels


def evaluate_clustering(features, labels):
    """
    ä½¿ç”¨DBSCANè¿›è¡Œèšç±»è¯„ä¼°ï¼ˆè‡ªåŠ¨ç¡®å®šèšç±»æ•°ï¼‰
    ç›´æ¥åœ¨2ç»´UMAPç©ºé—´è¿›è¡Œèšç±»ï¼Œä¿è¯å¯è§†åŒ–å’Œè¯„ä»·æŒ‡æ ‡ä¸€è‡´
    
    Args:
        features: ç‰¹å¾çŸ©é˜µ
        labels: çœŸå®æ ‡ç­¾
    """
    n_true_classes = len(np.unique(labels))
    
    # ç›´æ¥é™ç»´åˆ°2ç»´ï¼Œä¿è¯å¯è§†åŒ–å’Œè¯„ä»·æŒ‡æ ‡ä¸€è‡´
    print(f"  UMAP dimensionality reduction to 2D (True classes={n_true_classes})...")
    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    features_2d = reducer.fit_transform(features)
    
    # è‡ªåŠ¨ä¼°è®¡epså‚æ•°ï¼ˆä½¿ç”¨æ”¹è¿›çš„k-distanceæ–¹æ³•ï¼‰
    print(f"  Estimating eps parameter...")
    
    # è°ƒæ•´min_samplesï¼šæ ·æœ¬è¶Šå¤šï¼Œå¯ä»¥æ›´å¤§
    n_samples = len(features_2d)
    min_samples = max(10, min(n_samples // 100, n_true_classes * 5))
    
    # è®¡ç®—k-è·ç¦»
    nbrs = NearestNeighbors(n_neighbors=min_samples).fit(features_2d)
    distances, _ = nbrs.kneighbors(features_2d)
    k_distances = np.sort(distances[:, -1])
    
    # ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ï¼ˆelbowï¼‰è‡ªåŠ¨é€‰æ‹©eps
    # æ‰¾åˆ°k-distanceæ›²çº¿ä¸­æ–œç‡å˜åŒ–æœ€å¤§çš„ç‚¹
    # ä½¿ç”¨95-98åˆ†ä½æ•°ä¹‹é—´çš„å€¼ï¼Œé¿å…è¿‡åº¦ç¢ç‰‡åŒ–
    eps_candidates = [
        np.percentile(k_distances, 95),
        np.percentile(k_distances, 96),
        np.percentile(k_distances, 97),
        np.percentile(k_distances, 98)
    ]
    
    # é€‰æ‹©ä¸­ä½æ•°ä½œä¸ºepsï¼ˆæ›´ç¨³å¥ï¼‰
    eps = np.median(eps_candidates)
    
    print(f"  Running DBSCAN on 2D space (eps={eps:.4f}, min_samples={min_samples})...")
    
    # DBSCANèšç±»ï¼ˆåœ¨2ç»´ç©ºé—´ï¼‰ï¼Œå¸¦è‡ªé€‚åº”è°ƒæ•´
    max_attempts = 3
    for attempt in range(max_attempts):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
        pred_labels = dbscan.fit_predict(features_2d)
        
        # ç»Ÿè®¡èšç±»ç»“æœ
        n_clusters = len(set(pred_labels)) - (1 if -1 in pred_labels else 0)
        n_noise = list(pred_labels).count(-1)
        
        print(f"  â†’ Attempt {attempt+1}: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(pred_labels)*100:.1f}%)")
        
        # æ£€æŸ¥èšç±»æ•°æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡çœŸå®ç±»åˆ«æ•°çš„3å€ï¼‰
        if n_clusters <= n_true_classes * 3:
            print(f"  âœ“ Cluster count is reasonable (â‰¤ {n_true_classes * 3})")
            break
        elif attempt < max_attempts - 1:
            # èšç±»æ•°è¿‡å¤šï¼Œå¢å¤§epsï¼ˆæ”¾å®½å¯†åº¦è¦æ±‚ï¼‰
            eps *= 1.3
            print(f"  âš  Too many clusters, increasing eps to {eps:.4f}...")
        else:
            print(f"  âš  Warning: Still {n_clusters} clusters after {max_attempts} attempts")
    
    # å¤„ç†å™ªå£°ç‚¹ï¼šå°†å™ªå£°ç‚¹(-1)åˆ†é…ç»™æœ€è¿‘çš„ç°‡
    if n_noise > 0:
        # æ‰¾åˆ°æ‰€æœ‰éå™ªå£°ç‚¹
        valid_mask = pred_labels != -1
        valid_features = features_2d[valid_mask]
        valid_labels = pred_labels[valid_mask]
        
        # å¯¹å™ªå£°ç‚¹è¿›è¡Œkè¿‘é‚»åˆ†ç±»
        noise_mask = pred_labels == -1
        noise_features = features_2d[noise_mask]
        
        if len(valid_features) > 0:
            knn = KNeighborsClassifier(n_neighbors=min(5, len(valid_features)))
            knn.fit(valid_features, valid_labels)
            pred_labels[noise_mask] = knn.predict(noise_features)
            print(f"  Reassigned noise points to nearest clusters")
    
    # é‡æ–°æ˜ å°„æ ‡ç­¾ä¸ºè¿ç»­æ•´æ•°ï¼ˆ0, 1, 2, ...ï¼‰
    unique_labels = np.unique(pred_labels)
    label_mapping = {old: new for new, old in enumerate(unique_labels)}
    pred_labels = np.array([label_mapping[lbl] for lbl in pred_labels])
    n_clusters = len(unique_labels)  # æ›´æ–°èšç±»æ•°
    
    # è®¡ç®—æŒ‡æ ‡ï¼ˆåœ¨2ç»´ç©ºé—´ï¼‰
    ari = adjusted_rand_score(labels, pred_labels)
    nmi = normalized_mutual_info_score(labels, pred_labels)
    
    # åªæœ‰å½“èšç±»æ•°>1æ—¶æ‰è®¡ç®—è½®å»“ç³»æ•°
    if n_clusters > 1:
        sil = silhouette_score(features_2d, pred_labels)
    else:
        sil = 0.0
    
    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆé€šè¿‡N-to-1æ˜ å°„ï¼‰
    acc = cluster_accuracy(labels, pred_labels)
    
    return {
        'acc': acc,
        'ari': ari,
        'nmi': nmi,
        'sil': sil,
        'pred_labels': pred_labels,
        'n_clusters': n_clusters,
        'n_true_classes': n_true_classes,
        'features_reduced': features_2d  # è¿”å›2ç»´ç‰¹å¾ï¼Œç›´æ¥ç”¨äºå¯è§†åŒ–
    }


def cluster_accuracy(y_true, y_pred):
    """
    è®¡ç®—èšç±»å‡†ç¡®ç‡ï¼ˆå¤šå¯¹ä¸€æ˜ å°„ï¼ŒN-to-1ï¼‰
    å‚è€ƒ benchmark.py ä¸­çš„å®ç°
    
    æ¯ä¸ªèšç±»ç°‡æ˜ å°„åˆ°åŒ…å«æœ€å¤šæ ·æœ¬çš„çœŸå®ç±»åˆ«ï¼Œå…è®¸å¤šä¸ªç°‡æ˜ å°„åˆ°åŒä¸€ç±»åˆ«
    """
    y_true = y_true.astype(np.int64)
    y_pred = y_pred.astype(np.int64)
    
    # æ„å»ºæ··æ·†çŸ©é˜µï¼šcm[true_label, pred_cluster]
    cm = confusion_matrix(y_true, y_pred)
    
    # å¤šå¯¹ä¸€æ˜ å°„ï¼šæ¯ä¸ªèšç±»ç°‡æ˜ å°„åˆ°åŒ…å«æœ€å¤šæ ·æœ¬çš„çœŸå®ç±»åˆ«
    n_clusters = len(np.unique(y_pred))
    best_mapping = {}
    for cluster_id in range(n_clusters):
        # æ‰¾å‡ºè¯¥ç°‡ä¸­æœ€å¤šçš„çœŸå®ç±»åˆ«
        best_class = np.argmax(cm[:, cluster_id])
        best_mapping[cluster_id] = best_class
    
    # æ ¹æ®æ˜ å°„è®¡ç®—å‡†ç¡®ç‡
    mapped_pred = np.array([best_mapping[p] for p in y_pred])
    accuracy = np.mean(mapped_pred == y_true)
    
    return accuracy


def visualize_results(features, labels, pred_labels, save_path, method='umap'):
    """å¯è§†åŒ–èšç±»ç»“æœ"""
    # é™ç»´åˆ°2D
    if method == 'umap':
        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
        features_2d = reducer.fit_transform(features)
    else:  # tsne
        tsne = TSNE(n_components=2, random_state=42)
        features_2d = tsne.fit_transform(features)
    
    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # çœŸå®æ ‡ç­¾
    scatter1 = ax1.scatter(features_2d[:, 0], features_2d[:, 1], 
                          c=labels, cmap='tab10', s=5, alpha=0.6)
    ax1.set_title('True Labels', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Dimension 1')
    ax1.set_ylabel('Dimension 2')
    plt.colorbar(scatter1, ax=ax1)
    
    # èšç±»ç»“æœ
    scatter2 = ax2.scatter(features_2d[:, 0], features_2d[:, 1], 
                          c=pred_labels, cmap='tab10', s=5, alpha=0.6)
    ax2.set_title('Clustering Results', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Dimension 1')
    ax2.set_ylabel('Dimension 2')
    plt.colorbar(scatter2, ax=ax2)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {save_path}")


def plot_training_curve(losses, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(10, 6))
    plt.plot(losses, linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('TCCL Training Curve', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def run_single_dataset(dataset_name, data_root, epochs=50, batch_size=128, 
                       window_size=1024, stride=512, feature_dim=128, 
                       kernel_width=5, temperature=0.1, template_lambda=0.5,
                       lr=0.001, weight_decay=1e-6, device_str='cuda', seed=42,
                       patience=5, min_delta=0.0001, early_stop=True):
    """è¿è¡Œå•ä¸ªæ•°æ®é›†çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œè¿”å›ç»“æœ"""
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯å¤ç°
    set_random_seed(seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(device_str if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*80}")
    print(f"ğŸš€ TCCLå®éªŒ - {dataset_name}")
    print(f"{'='*80}")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"{'='*80}\n")
    
    # ========================================================================
    # 1. åŠ è½½æ•°æ®é›†
    # ========================================================================
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    
    if dataset_name == 'CWRU':
        train_dataset = CWRUDataset(
            data_root, 
            window_size=window_size,
            stride=stride,
            augment=True,
            verbose=True
        )
        eval_dataset = CWRUDataset(
            data_root,
            window_size=window_size,
            stride=stride,
            augment=False,
            verbose=False  # ä¸é‡å¤è¾“å‡º
        )
    elif dataset_name == 'SEU':
        train_dataset = SEUDataset(
            data_root,
            window_size=window_size,
            stride=stride,
            augment=True,
            verbose=True
        )
        eval_dataset = SEUDataset(
            data_root,
            window_size=window_size,
            stride=stride,
            augment=False,
            verbose=False  # ä¸é‡å¤è¾“å‡º
        )
    else:  # MFPT
        train_dataset = MFPTDataset(
            data_root,
            window_size=window_size,
            stride=stride,
            augment=True,
            verbose=True
        )
        eval_dataset = MFPTDataset(
            data_root,
            window_size=window_size,
            stride=stride,
            augment=False,
            verbose=False  # ä¸é‡å¤è¾“å‡º
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Windowsä¸‹è®¾ä¸º0é¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜
        pin_memory=True
    )
    
    eval_loader = DataLoader(
        eval_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,  # Windowsä¸‹è®¾ä¸º0é¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜
        pin_memory=True
    )
    
    n_classes = len(np.unique(train_dataset.labels))
    print(f"âœ“ æ•°æ®é›†åŠ è½½å®Œæˆï¼ç±»åˆ«æ•°: {n_classes}\n")
    
    # ========================================================================
    # 2. åˆ›å»ºæ¨¡å‹
    # ========================================================================
    print("ğŸ”¨ åˆ›å»ºTCCLæ¨¡å‹...")
    
    model = TCCLModel(
        feature_dim=feature_dim,
        kernel_width=kernel_width,
        temperature=temperature,
        template_lambda=template_lambda
    ).to(device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    n_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆï¼å‚æ•°é‡: {n_params:,}\n")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=lr,
        weight_decay=weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs
    )
    
    # ========================================================================
    # 3. è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ—©åœï¼‰
    # ========================================================================
    if early_stop:
        print(f"ğŸ“ å¼€å§‹è®­ç»ƒï¼ˆæ—©åœæœºåˆ¶: patience={patience}, min_delta={min_delta}ï¼‰...\n")
    else:
        print("ğŸ“ å¼€å§‹è®­ç»ƒï¼ˆæ—©åœæœºåˆ¶å·²ç¦ç”¨ï¼‰...\n")
    
    losses = []
    
    # æ—©åœå‚æ•°
    best_loss = float('inf')
    patience_counter = 0
    best_epoch = 0
    best_model_state = None
    early_stop_enabled = early_stop  # æ˜¯å¦å¯ç”¨æ—©åœ
    
    for epoch in range(1, epochs + 1):
        loss = train_tccl(model, train_loader, optimizer, device, epoch)
        losses.append(loss)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        if loss < best_loss - min_delta:
            best_loss = loss
            best_epoch = epoch
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ˆæ·±æ‹·è´ï¼‰
            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            status = "âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ï¼"
        else:
            patience_counter += 1
            status = f"(æ— æ”¹è¿›: {patience_counter}/{patience})"
        
        # scheduler.step()
        
        print(f"Epoch {epoch}/{epochs} - Loss: {loss:.4f} - Best: {best_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f} {status}")
        
        # æ—©åœæ£€æŸ¥
        if early_stop_enabled and patience_counter >= patience:
            print(f"\nâš ï¸ æ—©åœè§¦å‘ï¼è¿ç»­{patience}ä¸ªepochæ— æ”¹è¿›")
            print(f"   æœ€ä½³Epoch: {best_epoch}, æœ€ä½³Loss: {best_loss:.4f}")
            break
    
    # æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡
    if best_model_state is not None:
        print(f"\nğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡ (Epoch {best_epoch}, Loss {best_loss:.4f})")
        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}, æœ€ä½³æŸå¤±: {best_loss:.4f}\n")
    
    # ========================================================================
    # ä¿å­˜æ¨¡å‹
    # ========================================================================
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    model_dir = './saved_models'
    os.makedirs(model_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŒ…æ‹¬ç»“æ„å’Œæƒé‡ï¼‰
    model_path = os.path.join(model_dir, f'tccl_{dataset_name.lower()}_best.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': best_epoch,  # ä¿å­˜æœ€ä½³epoch
        'loss': best_loss,    # ä¿å­˜æœ€ä½³loss
        'final_epoch': len(losses),  # å®é™…è®­ç»ƒçš„epochæ•°
        'final_loss': losses[-1],     # æœ€ç»ˆçš„loss
        'feature_dim': feature_dim,
        'kernel_width': kernel_width,
        'temperature': temperature,
        'template_lambda': template_lambda,
        'dataset_name': dataset_name
    }, model_path)
    
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}\n")
    
    # ========================================================================
    # 4. è¯„ä¼°æ¨¡å‹
    # ========================================================================
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...\n")
    
    # æå–ç‰¹å¾
    features, labels = extract_all_features(model, eval_loader, device)
    print(f"âœ“ ç‰¹å¾æå–å®Œæˆï¼å½¢çŠ¶: {features.shape}\n")
    
    # èšç±»è¯„ä¼°ï¼ˆä½¿ç”¨DBSCANè‡ªåŠ¨ç¡®å®šèšç±»æ•°ï¼‰
    print("æ‰§è¡Œèšç±»è¯„ä¼°...")
    results = evaluate_clustering(features, labels)
    
    print("\n" + "="*80)
    print("ğŸ“Š èšç±»æ€§èƒ½ç»“æœ")
    print("="*80)
    print(f"  èšç±»æ•°/çœŸå®ç±»åˆ«æ•°: {results['n_clusters']}/{results['n_true_classes']} " + 
          f"({'=' if results['n_clusters'] == results['n_true_classes'] else 'N-to-1 æ˜ å°„'})")
    print(f"  Accuracy:  {results['acc']:.4f}")
    print(f"  ARI:       {results['ari']:.4f}")
    print(f"  NMI:       {results['nmi']:.4f}")
    print(f"  Silhouette: {results['sil']:.4f}")
    print("="*80 + "\n")
    
    # ========================================================================
    # 5. è·å–2Dç‰¹å¾ï¼ˆå·²åœ¨èšç±»è¯„ä¼°ä¸­ç”Ÿæˆï¼‰
    # ========================================================================
    # ç›´æ¥ä½¿ç”¨evaluate_clusteringè¿”å›çš„2Dç‰¹å¾ï¼Œä¿è¯å¯è§†åŒ–å’Œè¯„ä»·æŒ‡æ ‡ä¸€è‡´
    features_2d = results['features_reduced']
    
    print("="*80)
    print(f"âœ… {dataset_name} å®éªŒå®Œæˆï¼")
    print("="*80)
    
    # è¿”å›ç»“æœä¾›åç»­å¯¹æ¯”
    return {
        'dataset': dataset_name,
        'features': features,
        'features_2d': features_2d,
        'labels': labels,
        'pred_labels': results['pred_labels'],
        'metrics': results,
        'losses': losses,  # å®Œæ•´çš„æŸå¤±å†å²
        'final_loss': losses[-1]
    }


def compare_datasets(results_list):
    """Compare UMAP visualization results of three datasets (Ground Truth vs Clustering vs Loss)"""
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    
    for idx, result in enumerate(results_list):
        dataset_name = result['dataset']
        features_2d = result['features_2d']
        labels = result['labels']
        pred_labels = result['pred_labels']
        metrics = result['metrics']
        losses = result['losses']
        
        # Column 1: Ground Truth
        ax_true = axes[idx, 0]
        unique_labels = np.unique(labels)
        colors_true = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = (labels == label)
            ax_true.scatter(features_2d[mask, 0], features_2d[mask, 1],
                          c=[colors_true[i]], label=f'True Class {label}',
                          s=20, alpha=0.7, edgecolors='black', linewidth=0.3)
        
        ax_true.set_title(f'{dataset_name} - Ground Truth', fontsize=12, fontweight='bold')
        ax_true.set_xlabel('UMAP 1', fontsize=10)
        ax_true.set_ylabel('UMAP 2', fontsize=10)
        ax_true.legend(loc='best', fontsize=8)
        ax_true.grid(True, alpha=0.3)
        
        # Column 2: Clustering Results
        ax_pred = axes[idx, 1]
        unique_clusters = np.unique(pred_labels)
        colors_pred = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))
        
        for i, cluster in enumerate(unique_clusters):
            mask = (pred_labels == cluster)
            ax_pred.scatter(features_2d[mask, 0], features_2d[mask, 1],
                          c=[colors_pred[i]], label=f'Cluster {cluster}',
                          s=20, alpha=0.7, edgecolors='black', linewidth=0.3)
        
        ax_pred.set_title(f'{dataset_name} - Clustering (K={metrics["n_clusters"]})\nAcc: {metrics["acc"]:.3f} | ARI: {metrics["ari"]:.3f} | NMI: {metrics["nmi"]:.3f}',
                         fontsize=12, fontweight='bold')
        ax_pred.set_xlabel('UMAP 1', fontsize=10)
        ax_pred.set_ylabel('UMAP 2', fontsize=10)
        ax_pred.legend(loc='best', fontsize=8)
        ax_pred.grid(True, alpha=0.3)
        
        # Column 3: Training Loss Curve
        ax_loss = axes[idx, 2]
        epochs = range(1, len(losses) + 1)
        ax_loss.plot(epochs, losses, linewidth=2, color='#e74c3c', marker='o', 
                    markersize=3, markevery=max(1, len(losses)//20))
        
        # æ ‡æ³¨åˆå§‹å’Œæœ€ç»ˆæŸå¤±
        ax_loss.text(1, losses[0], f'{losses[0]:.3f}', 
                    fontsize=9, va='bottom', ha='right', color='#e74c3c')
        ax_loss.text(len(losses), losses[-1], f'{losses[-1]:.3f}', 
                    fontsize=9, va='bottom', ha='left', color='#e74c3c')
        
        # åˆ¤æ–­æ”¶æ•›æ€§
        improvement = (losses[0] - losses[-1]) / losses[0] * 100
        converged = "âœ“" if improvement > 10 else "?"
        
        ax_loss.set_title(f'{dataset_name} - Training Loss {converged}\nImprovement: {improvement:.1f}%',
                         fontsize=12, fontweight='bold')
        ax_loss.set_xlabel('Epoch', fontsize=10)
        ax_loss.set_ylabel('Loss', fontsize=10)
        ax_loss.grid(True, alpha=0.3)
        ax_loss.set_xlim(0, len(losses) + 1)
    
    plt.suptitle('TCCL Results - Three Datasets (Ground Truth | Clustering | Training Loss)', 
                 fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.show()
    print(f"\nâœ“ Comparison plot displayed")


def main(seed=42, epochs=100, batch_size=128, lr=0.0005, temperature=0.5, 
         template_lambda=0.5, feature_dim=128, device_str='cuda', 
         cwru_path=None, seu_path=None, mfpt_path=None,
         patience=15, min_delta=0.0001, early_stop=True):
    """ä¸»å‡½æ•°ï¼šä¾æ¬¡è¿è¡Œä¸‰ä¸ªæ•°æ®é›†å¹¶å¯¹æ¯”"""
    
    # ========================================================================
    # æ•°æ®é›†é…ç½®
    # ========================================================================
    datasets_config = [
        {
            'name': 'CWRU',
            'path': cwru_path or 'E:/AI/CWRU-dataset-main/48k007'
        },
        {
            'name': 'SEU', 
            'path': seu_path or 'E:/AI/Mechanical-datasets-master/dataset'
        },
        {
            'name': 'MFPT',
            'path': mfpt_path or 'E:/AI/MFPT-Fault-Data-Sets-20200227T131140Z-001/MFPT/MFPT'
        }
    ]
    
    # ç»Ÿä¸€çš„è®­ç»ƒå‚æ•°
    train_config = {
        'epochs': epochs,
        'batch_size': batch_size,
        'window_size': 1024,
        'stride': 512,
        'feature_dim': feature_dim,
        'kernel_width': 5,  # æ”¹ä¸º5ï¼Œå¢å¼ºæ¨¡æ¿è¡¨è¾¾èƒ½åŠ›
        'temperature': temperature,
        'template_lambda': template_lambda,  # æ¨¡æ¿å¯¹æ¯”æŸå¤±æƒé‡
        'lr': lr,
        'weight_decay': 1e-4,
        'device_str': device_str,
        'seed': seed,                # éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
        'patience': patience,        # æ—©åœè€å¿ƒå€¼
        'min_delta': min_delta,      # æœ€å°æ”¹è¿›é˜ˆå€¼
        'early_stop': early_stop     # æ˜¯å¦å¯ç”¨æ—©åœ
    }
    
    print("\n" + "="*100)
    print("ğŸš€ TCCL ä¸‰æ•°æ®é›†æ‰¹é‡å®éªŒ")
    print("="*100)
    print(f"å°†ä¾æ¬¡è¿è¡Œ: {', '.join([d['name'] for d in datasets_config])}")
    print(f"éšæœºç§å­: {train_config['seed']}")
    print(f"è®­ç»ƒè½®æ•°: {train_config['epochs']}")
    print(f"æ‰¹æ¬¡å¤§å°: {train_config['batch_size']}")
    print(f"å­¦ä¹ ç‡: {train_config['lr']}")
    print(f"æ¸©åº¦å‚æ•°: {train_config['temperature']}")
    print(f"æ¨¡æ¿æŸå¤±æƒé‡: {train_config['template_lambda']}")
    print(f"ç‰¹å¾ç»´åº¦: {train_config['feature_dim']}")
    print(f"æ—©åœæœºåˆ¶: {'å¯ç”¨' if train_config['early_stop'] else 'ç¦ç”¨'} (patience={train_config['patience']}, min_delta={train_config['min_delta']})")
    print("="*100)
    
    # ========================================================================
    # ä¾æ¬¡è¿è¡Œä¸‰ä¸ªæ•°æ®é›†
    # ========================================================================
    all_results = []
    
    for dataset_cfg in datasets_config:
        result = run_single_dataset(
            dataset_name=dataset_cfg['name'],
            data_root=dataset_cfg['path'],
            **train_config
        )
        all_results.append(result)
    
    # ========================================================================
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    # ========================================================================
    print("\n" + "="*115)
    print("ğŸ“Š ä¸‰æ•°æ®é›†æ€§èƒ½æ±‡æ€»")
    print("="*115)
    print(f"{'æ•°æ®é›†':<15} {'K/C':>6} {'Accuracy':>10} {'ARI':>10} {'NMI':>10} {'Silhouette':>12} {'Loss':>10}")
    print(f"{'':15} {'':6} {'(N-to-1)':>10}")
    print("-"*115)
    
    for result in all_results:
        dataset = result['dataset']
        m = result['metrics']
        loss = result['final_loss']
        k_ratio = f"{m['n_clusters']}/{m['n_true_classes']}"
        print(f"{dataset:<15} {k_ratio:>6} {m['acc']:>10.4f} {m['ari']:>10.4f} {m['nmi']:>10.4f} "
              f"{m['sil']:>12.4f} {loss:>10.4f}")
    
    print("="*115)
    print("æ³¨: K/C = èšç±»æ•°/çœŸå®ç±»åˆ«æ•°, K>C è¡¨ç¤ºä½¿ç”¨äº† N-to-1 æ˜ å°„")
    
    # ========================================================================
    # ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–ï¼ˆæœ€åå±•ç¤ºï¼‰
    # ========================================================================
    print("\n" + "="*100)
    print("ğŸ“Š å±•ç¤ºä¸‰æ•°æ®é›†UMAPå¯¹æ¯”å¯è§†åŒ–...")
    print("="*100)
    
    compare_datasets(all_results)
    
    print("\n" + "="*100)
    print("âœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("="*100)
    

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='TCCLä¸‰æ•°æ®é›†æ‰¹é‡å®éªŒ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--seed', type=int, default=42, 
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 100, æ¨è: 100-200)')
    parser.add_argument('--batch_size', type=int, default=128, 
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 128)')
    parser.add_argument('--lr', type=float, default=0.0005, 
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.0005, æ¨èèŒƒå›´: 0.0003-0.001)')
    parser.add_argument('--temperature', type=float, default=0.5, 
                       help='æ¸©åº¦å‚æ•° (é»˜è®¤: 0.5, æ¨èèŒƒå›´: 0.3-0.7)')
    parser.add_argument('--template_lambda', type=float, default=0.5,
                       help='æ¨¡æ¿å¯¹æ¯”æŸå¤±æƒé‡ (é»˜è®¤: 0.5, æ¨èèŒƒå›´: 0.3-1.0)')
    parser.add_argument('--feature_dim', type=int, default=128, 
                       help='ç‰¹å¾ç»´åº¦ (é»˜è®¤: 128, æ¨è: 64/128/256)')
    parser.add_argument('--device', type=str, default='cuda', 
                       help='è®¾å¤‡ (é»˜è®¤: cuda)')
    
    # æ—©åœå‚æ•°
    parser.add_argument('--patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 15, è¿ç»­Nä¸ªepochæ— æ”¹è¿›åˆ™åœæ­¢)')
    parser.add_argument('--min_delta', type=float, default=0.0001,
                       help='æœ€å°æ”¹è¿›é˜ˆå€¼ (é»˜è®¤: 0.0001)')
    parser.add_argument('--no_early_stop', action='store_true',
                       help='ç¦ç”¨æ—©åœæœºåˆ¶ (é»˜è®¤: å¯ç”¨)')
    
    # æ•°æ®é›†è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
    parser.add_argument('--cwru_path', type=str, 
                       default='E:/AI/CWRU-dataset-main/48k007',
                       help='CWRUæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--seu_path', type=str, 
                       default='E:/AI/Mechanical-datasets-master/dataset',
                       help='SEUæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--mfpt_path', type=str, 
                       default='E:/AI/MFPT-Fault-Data-Sets-20200227T131140Z-001/MFPT/MFPT',
                       help='MFPTæ•°æ®é›†è·¯å¾„')
    
    return parser.parse_args()


if __name__ == '__main__':
    args = parse_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
    main(seed=args.seed, epochs=args.epochs, batch_size=args.batch_size,
         lr=args.lr, temperature=args.temperature, template_lambda=args.template_lambda,
         feature_dim=args.feature_dim, device_str=args.device, 
         cwru_path=args.cwru_path, seu_path=args.seu_path, mfpt_path=args.mfpt_path,
         patience=args.patience, min_delta=args.min_delta, 
         early_stop=not args.no_early_stop)

