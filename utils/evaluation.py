"""
è¯„ä¼°æ¨¡å— - æä¾›å…¨é¢çš„èšç±»è¯„ä¼°æŒ‡æ ‡

åŠŸèƒ½åŒ…æ‹¬:
- æ— ç›‘ç£èšç±»æŒ‡æ ‡ï¼šSilhouetteã€Davies-Bouldinã€Calinski-Harabasz
- æœ‰ç›‘ç£èšç±»æŒ‡æ ‡ï¼šARIã€NMI
- èšç±»å‡†ç¡®ç‡ï¼š1å¯¹1æ˜ å°„ã€å¤šå¯¹1æ˜ å°„
- åˆ†ç¦»åº¦æŒ‡æ ‡ï¼šç±»å†…/ç±»é—´è·ç¦»ã€åˆ†ç¦»æ¯”
- æœ€ä¼˜èšç±»æ•°æœç´¢
"""

import numpy as np
import torch
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    adjusted_rand_score,
    normalized_mutual_info_score,
    confusion_matrix,
    pairwise_distances
)
from scipy.optimize import linear_sum_assignment
from typing import Dict, Tuple, Optional, List
import umap.umap_ as umap
from tqdm import tqdm


class ClusteringEvaluator:
    """èšç±»è¯„ä¼°å™¨"""
    
    def __init__(self, n_clusters: Optional[int] = None, use_umap: bool = True,
                 umap_params: Optional[Dict] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            n_clusters: èšç±»æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æœç´¢æœ€ä¼˜K
            use_umap: æ˜¯å¦ä½¿ç”¨UMAPé™ç»´åˆ°2Dè¿›è¡Œèšç±»
            umap_params: UMAPå‚æ•°å­—å…¸
        """
        self.n_clusters = n_clusters
        self.use_umap = use_umap
        
        # é»˜è®¤UMAPå‚æ•°
        if umap_params is None:
            self.umap_params = {
                'n_neighbors': 20,
                'min_dist': 0.1,
                'metric': 'cosine',
                'random_state': 42
            }
        else:
            self.umap_params = umap_params
        
        self.reducer = None
        self.kmeans = None
        self.scaler = StandardScaler()
    
    def fit_transform_umap(self, features: np.ndarray) -> np.ndarray:
        """
        ä½¿ç”¨UMAPé™ç»´åˆ°2D
        
        Args:
            features: é«˜ç»´ç‰¹å¾ [n_samples, n_features]
        
        Returns:
            features_2d: 2Dç‰¹å¾ [n_samples, 2]
        """
        print("  Fitting UMAP...")
        self.reducer = umap.UMAP(**self.umap_params)
        features_2d = self.reducer.fit_transform(features)
        return features_2d
    
    def find_optimal_k(self, features: np.ndarray, k_range: range = None) -> int:
        """
        ä½¿ç”¨è½®å»“ç³»æ•°å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
            k_range: æœç´¢èŒƒå›´
        
        Returns:
            optimal_k: æœ€ä¼˜èšç±»æ•°
        """
        if k_range is None:
            k_range = range(2, min(20, len(features) // 10))
        
        print(f"  Searching optimal K in range {list(k_range)[:3]}...{list(k_range)[-3:]}...")
        
        silhouettes = []
        for k in tqdm(k_range, desc="  Finding optimal K", leave=False):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            pred_labels = kmeans.fit_predict(features)
            silhouettes.append(silhouette_score(features, pred_labels))
        
        optimal_k = list(k_range)[np.argmax(silhouettes)]
        print(f"  âœ“ Optimal K: {optimal_k} (Silhouette: {max(silhouettes):.4f})")
        
        return optimal_k
    
    def fit_kmeans(self, features: np.ndarray, n_clusters: int) -> np.ndarray:
        """
        æ‰§è¡ŒK-Meansèšç±»
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
            n_clusters: èšç±»æ•°
        
        Returns:
            pred_labels: é¢„æµ‹æ ‡ç­¾ [n_samples]
        """
        print(f"  Running K-Means (K={n_clusters})...")
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)
        pred_labels = self.kmeans.fit_predict(features)
        return pred_labels
    
    def compute_unsupervised_metrics(
        self, 
        features: np.ndarray, 
        pred_labels: np.ndarray
    ) -> Dict[str, float]:
        """
        è®¡ç®—æ— ç›‘ç£èšç±»æŒ‡æ ‡
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ
            pred_labels: é¢„æµ‹æ ‡ç­¾
        
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # Silhouetteç³»æ•° (è¶Šå¤§è¶Šå¥½ï¼ŒèŒƒå›´[-1, 1])
        metrics['silhouette'] = silhouette_score(features, pred_labels)
        
        # Davies-BouldinæŒ‡æ•° (è¶Šå°è¶Šå¥½)
        metrics['davies_bouldin'] = davies_bouldin_score(features, pred_labels)
        
        # Calinski-HarabaszæŒ‡æ•° (è¶Šå¤§è¶Šå¥½)
        metrics['calinski_harabasz'] = calinski_harabasz_score(features, pred_labels)
        
        return metrics
    
    def compute_supervised_metrics(
        self,
        true_labels: np.ndarray,
        pred_labels: np.ndarray
    ) -> Dict[str, float]:
        """
        è®¡ç®—æœ‰ç›‘ç£èšç±»æŒ‡æ ‡
        
        Args:
            true_labels: çœŸå®æ ‡ç­¾
            pred_labels: é¢„æµ‹æ ‡ç­¾
        
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # Adjusted Rand Index (èŒƒå›´[-1, 1]ï¼Œè¶Šå¤§è¶Šå¥½)
        metrics['ari'] = adjusted_rand_score(true_labels, pred_labels)
        
        # Normalized Mutual Information (èŒƒå›´[0, 1]ï¼Œè¶Šå¤§è¶Šå¥½)
        metrics['nmi'] = normalized_mutual_info_score(true_labels, pred_labels)
        
        return metrics
    
    def compute_accuracy(
        self,
        true_labels: np.ndarray,
        pred_labels: np.ndarray
    ) -> Dict[str, any]:
        """
        è®¡ç®—èšç±»å‡†ç¡®ç‡ï¼ˆéœ€è¦æ ‡ç­¾æ˜ å°„ï¼‰
        
        Args:
            true_labels: çœŸå®æ ‡ç­¾
            pred_labels: é¢„æµ‹æ ‡ç­¾
        
        Returns:
            accuracy_metrics: åŒ…å«å‡†ç¡®ç‡å’Œæ˜ å°„å…³ç³»çš„å­—å…¸
        """
        metrics = {}
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, pred_labels)
        n_true_classes = len(np.unique(true_labels))
        n_pred_clusters = len(np.unique(pred_labels))
        
        # 1å¯¹1æ˜ å°„ï¼ˆä»…å½“ç°‡æ•°ç­‰äºç±»æ•°æ—¶ï¼‰
        if n_pred_clusters == n_true_classes:
            row_ind, col_ind = linear_sum_assignment(cm, maximize=True)
            accuracy_1to1 = cm[row_ind, col_ind].sum() / len(true_labels)
            metrics['accuracy_1to1'] = accuracy_1to1
            metrics['mapping_1to1'] = dict(zip(col_ind, row_ind))
        else:
            metrics['accuracy_1to1'] = None
            metrics['mapping_1to1'] = None
        
        # å¤šå¯¹1æ˜ å°„ï¼ˆN-to-1ï¼‰
        best_mapping = {}
        for cluster_id in range(n_pred_clusters):
            best_class = np.argmax(cm[:, cluster_id])
            best_mapping[cluster_id] = best_class
        
        mapped_pred = np.array([best_mapping[p] for p in pred_labels])
        accuracy_Nto1 = np.mean(mapped_pred == true_labels)
        
        metrics['accuracy_Nto1'] = accuracy_Nto1
        metrics['mapping_Nto1'] = best_mapping
        metrics['confusion_matrix'] = cm
        
        return metrics
    
    def compute_separation_metrics(
        self,
        features: np.ndarray,
        pred_labels: np.ndarray
    ) -> Dict[str, float]:
        """
        è®¡ç®—ç±»å†…/ç±»é—´è·ç¦»å’Œåˆ†ç¦»æ¯”
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ
            pred_labels: é¢„æµ‹æ ‡ç­¾
        
        Returns:
            metrics: åˆ†ç¦»åº¦æŒ‡æ ‡å­—å…¸
        """
        n_clusters = len(np.unique(pred_labels))
        
        # è®¡ç®—ç±»å†…è·ç¦»ï¼ˆå¹³å‡ï¼‰
        intra_cluster_dist = 0
        for cluster_id in range(n_clusters):
            cluster_mask = (pred_labels == cluster_id)
            if cluster_mask.sum() > 0:
                cluster_features = features[cluster_mask]
                center = cluster_features.mean(axis=0, keepdims=True)
                intra_cluster_dist += np.mean(pairwise_distances(cluster_features, center))
        intra_cluster_dist /= n_clusters
        
        # è®¡ç®—ç±»é—´è·ç¦»ï¼ˆèšç±»ä¸­å¿ƒä¹‹é—´çš„å¹³å‡è·ç¦»ï¼‰
        centers = np.array([features[pred_labels == i].mean(axis=0) 
                           for i in range(n_clusters)])
        inter_cluster_dist = np.mean(pairwise_distances(centers))
        
        # åˆ†ç¦»æ¯”ï¼ˆç±»é—´è·ç¦»/ç±»å†…è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½ï¼‰
        separation_ratio = inter_cluster_dist / (intra_cluster_dist + 1e-8)
        
        metrics = {
            'intra_dist': intra_cluster_dist,
            'inter_dist': inter_cluster_dist,
            'separation_ratio': separation_ratio
        }
        
        return metrics
    
    def evaluate(
        self,
        features: np.ndarray,
        true_labels: np.ndarray,
        verbose: bool = True
    ) -> Dict[str, any]:
        """
        å®Œæ•´çš„èšç±»è¯„ä¼°æµç¨‹
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
            true_labels: çœŸå®æ ‡ç­¾ [n_samples]
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            results: åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å’Œä¸­é—´ç»“æœçš„å­—å…¸
        """
        if verbose:
            print("=" * 60)
            print("Clustering Evaluation")
            print("=" * 60)
            print(f"Features shape: {features.shape}")
            print(f"Number of samples: {len(features)}")
            print(f"Number of true classes: {len(np.unique(true_labels))}")
        
        results = {}
        
        # æ­¥éª¤1: UMAPé™ç»´ï¼ˆå¯é€‰ï¼‰
        if self.use_umap:
            features_2d = self.fit_transform_umap(features)
            features_to_cluster = features_2d
            results['features_2d'] = features_2d
        else:
            features_to_cluster = features
            results['features_2d'] = None
        
        # æ­¥éª¤2: ç‰¹å¾æ ‡å‡†åŒ–
        features_scaled = self.scaler.fit_transform(features_to_cluster)
        results['features_scaled'] = features_scaled
        
        # æ­¥éª¤3: ç¡®å®šèšç±»æ•°
        n_true_classes = len(np.unique(true_labels))
        if self.n_clusters is None:
            optimal_k = self.find_optimal_k(features_scaled)
        else:
            optimal_k = self.n_clusters
        
        results['n_clusters'] = optimal_k
        results['n_true_classes'] = n_true_classes
        
        # æ­¥éª¤4: K-Meansèšç±»
        pred_labels = self.fit_kmeans(features_scaled, optimal_k)
        results['pred_labels'] = pred_labels
        
        # æ­¥éª¤5: è®¡ç®—æ— ç›‘ç£æŒ‡æ ‡
        if verbose:
            print("\nğŸ“Š Computing metrics...")
        
        unsupervised_metrics = self.compute_unsupervised_metrics(
            features_scaled, pred_labels
        )
        results.update(unsupervised_metrics)
        
        # æ­¥éª¤6: è®¡ç®—æœ‰ç›‘ç£æŒ‡æ ‡
        supervised_metrics = self.compute_supervised_metrics(
            true_labels, pred_labels
        )
        results.update(supervised_metrics)
        
        # æ­¥éª¤7: è®¡ç®—å‡†ç¡®ç‡
        accuracy_metrics = self.compute_accuracy(
            true_labels, pred_labels
        )
        results.update(accuracy_metrics)
        
        # æ­¥éª¤8: è®¡ç®—åˆ†ç¦»åº¦
        separation_metrics = self.compute_separation_metrics(
            features_scaled, pred_labels
        )
        results.update(separation_metrics)
        
        # æ‰“å°ç»“æœ
        if verbose:
            print("\n" + "=" * 60)
            print("Evaluation Results")
            print("=" * 60)
            print(f"Optimal K: {optimal_k} (True K: {n_true_classes})")
            print(f"\nğŸ“Š Unsupervised Metrics:")
            print(f"  Silhouette Score:      {unsupervised_metrics['silhouette']:.4f}")
            print(f"  Davies-Bouldin Index:  {unsupervised_metrics['davies_bouldin']:.4f}")
            print(f"  Calinski-Harabasz:     {unsupervised_metrics['calinski_harabasz']:.1f}")
            
            print(f"\nğŸ“ˆ Supervised Metrics:")
            print(f"  Adjusted Rand Index:   {supervised_metrics['ari']:.4f}")
            print(f"  Normalized Mutual Info:{supervised_metrics['nmi']:.4f}")
            
            print(f"\nğŸ¯ Accuracy:")
            if accuracy_metrics['accuracy_1to1'] is not None:
                print(f"  1-to-1 Accuracy:       {accuracy_metrics['accuracy_1to1']:.4f}")
            print(f"  N-to-1 Accuracy:       {accuracy_metrics['accuracy_Nto1']:.4f}")
            
            print(f"\nğŸ” Separation Metrics:")
            print(f"  Intra-cluster Distance:{separation_metrics['intra_dist']:.4f}")
            print(f"  Inter-cluster Distance:{separation_metrics['inter_dist']:.4f}")
            print(f"  Separation Ratio:      {separation_metrics['separation_ratio']:.4f}")
            print("=" * 60)
        
        return results


def evaluate_model(
    model,
    dataloader,
    device: str = 'cuda',
    use_umap: bool = True,
    n_clusters: Optional[int] = None,
    verbose: bool = True
) -> Dict[str, any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°TCCLæ¨¡å‹
    
    Args:
        model: TCCLæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        use_umap: æ˜¯å¦ä½¿ç”¨UMAPé™ç»´
        n_clusters: èšç±»æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æœç´¢ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    # æå–ç‰¹å¾
    if verbose:
        print("Extracting features...")
    
    model.eval()
    all_features = []
    all_labels = []
    
    with torch.no_grad():
        for view1, _, labels in tqdm(dataloader, desc="Extracting", disable=not verbose):
            view1 = view1.to(device)
            features = model.extract_features(view1, normalize=True)
            all_features.append(features.cpu().numpy())
            all_labels.append(labels.numpy())
    
    features = np.concatenate(all_features, axis=0)
    labels = np.concatenate(all_labels, axis=0)
    
    if verbose:
        print(f"âœ“ Extracted {len(features)} samples with {features.shape[1]} dimensions")
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¯„ä¼°
    evaluator = ClusteringEvaluator(n_clusters=n_clusters, use_umap=use_umap)
    results = evaluator.evaluate(features, labels, verbose=verbose)
    
    # æ·»åŠ åŸå§‹ç‰¹å¾å’Œæ ‡ç­¾
    results['features'] = features
    results['labels'] = labels
    
    return results


def compare_models(
    models_dict: Dict[str, any],
    dataloader,
    device: str = 'cuda',
    use_umap: bool = True,
    verbose: bool = True
) -> Dict[str, Dict]:
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        models_dict: æ¨¡å‹å­—å…¸ {model_name: model}
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        use_umap: æ˜¯å¦ä½¿ç”¨UMAP
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        comparison_results: æ¯”è¾ƒç»“æœå­—å…¸
    """
    comparison_results = {}
    
    print("=" * 80)
    print("Comparing Multiple Models")
    print("=" * 80)
    
    for model_name, model in models_dict.items():
        print(f"\n{'='*80}")
        print(f"Evaluating: {model_name}")
        print(f"{'='*80}")
        
        results = evaluate_model(
            model, dataloader, device=device,
            use_umap=use_umap, verbose=verbose
        )
        
        comparison_results[model_name] = results
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\n" + "=" * 80)
    print("Performance Comparison")
    print("=" * 80)
    print(f"{'Model':<20} {'Accâ†‘':>8} {'ARIâ†‘':>8} {'NMIâ†‘':>8} {'Silâ†‘':>8} {'SepRâ†‘':>8} {'DBâ†“':>8}")
    print("-" * 80)
    
    for model_name, results in comparison_results.items():
        acc = results['accuracy_Nto1']
        ari = results['ari']
        nmi = results['nmi']
        sil = results['silhouette']
        sep = results['separation_ratio']
        db = results['davies_bouldin']
        
        print(f"{model_name:<20} {acc:>8.4f} {ari:>8.4f} {nmi:>8.4f} "
              f"{sil:>8.4f} {sep:>8.4f} {db:>8.4f}")
    
    print("=" * 80)
    
    return comparison_results


if __name__ == "__main__":
    """æµ‹è¯•ä»£ç """
    print("Testing Clustering Evaluator...")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    from sklearn.datasets import make_blobs
    
    X, y = make_blobs(n_samples=300, n_features=64, centers=4, random_state=42)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ClusteringEvaluator(n_clusters=None, use_umap=True)
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate(X, y, verbose=True)
    
    print("\nâœ“ Evaluation test passed!")

