"""
MFPTè½´æ‰¿æ•…éšœæ•°æ®é›† - æ”¹è¿›ç‰ˆ

æ”¯æŒä¸‰ç§æ ‡å‡†åŒ–æ¨¡å¼ï¼š
1. mode='full': ä½¿ç”¨å…¨æ•°æ®é›†ç»Ÿè®¡é‡æ ‡å‡†åŒ–ï¼ˆæ¨èç”¨äºæ— ç›‘ç£èšç±»ï¼‰
2. mode='train': è®­ç»ƒæ¨¡å¼ï¼Œè®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
3. mode='test': æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡

è®­ç»ƒå’Œæµ‹è¯•é›†éœ€è¦æ‰‹åŠ¨åˆ†å¼€å­˜å‚¨ï¼Œå¹¶ä¸”éœ€è¦åˆ†å¼€è®¡ç®—ç»Ÿè®¡é‡

æ ‡å‡†åŒ–ç­–ç•¥ï¼š
- æ•°æ®é›†çº§åˆ«çš„Z-scoreæ ‡å‡†åŒ–
- ä¿ç•™ä¸åŒæ–‡ä»¶é—´çš„æŒ¯å¹…å·®å¼‚ï¼ˆé‡è¦çš„æ•…éšœç‰¹å¾ï¼‰
- ç¡®ä¿è®­ç»ƒ-æµ‹è¯•ä¸€è‡´æ€§ï¼Œé¿å…æ•°æ®æ³„éœ²
"""

import torch
from torch.utils.data import Dataset
import scipy.io as sio
import numpy as np
from tqdm import tqdm
from pathlib import Path
from typing import Optional, Dict


def load_mfpt_signal(file_path: str) -> np.ndarray:
    """ä»MFPTæ ¼å¼çš„.matæ–‡ä»¶ä¸­åŠ è½½æŒ¯åŠ¨ä¿¡å·"""
    try:
        mat_contents = sio.loadmat(file_path)
        signal_vec = mat_contents['bearing']['gs'][0, 0].flatten()
        return signal_vec
    except Exception as e:
        print(f"è­¦å‘Š: åŠ è½½æ–‡ä»¶ {Path(file_path).name} å¤±è´¥: {e}")
        return np.array([])


class MFPTDataset(Dataset):
    """MFPTè½´æ‰¿æ•…éšœæ•°æ®é›†"""
    
    LABEL_MAP = {
        'normal': 0,
        'inner_race': 1,
        'outer_race': 2,
    }

    def __init__(self, 
                 root_dir: str, 
                 window_size: int = 1024,
                 step_size: int = 512, 
                 augmentor=None,
                 normalization_stats: Optional[Dict] = None,
                 mode: str = 'full'):
        """
        åˆå§‹åŒ–MFPTæ•°æ®é›†
        
        å‚æ•°:
            root_dir: æ•°æ®é›†æ ¹ç›®å½•ï¼ŒåŒ…å«.matæ–‡ä»¶
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            step_size: æ»‘åŠ¨çª—å£æ­¥é•¿
            augmentor: æ•°æ®å¢å¼ºå™¨ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
            normalization_stats: é¢„å…ˆè®¡ç®—çš„ç»Ÿè®¡é‡ {'mean': ..., 'std': ...}
            mode: æ¨¡å¼é€‰æ‹©
                - 'full': å…¨æ•°æ®é›†æ¨¡å¼ï¼Œè®¡ç®—å¹¶ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„ç»Ÿè®¡é‡ï¼ˆæ¨èç”¨äºæ— ç›‘ç£èšç±»ï¼‰
                - 'train': è®­ç»ƒæ¨¡å¼ï¼Œè®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
                - 'test': æµ‹è¯•æ¨¡å¼ï¼Œå¿…é¡»æä¾›normalization_stats
        """
        self.root_dir = Path(root_dir)
        self.window_size = window_size
        self.step_size = step_size
        self.augmentor = augmentor
        self.mode = mode
        self.class_names = {v: k for k, v in self.LABEL_MAP.items()}

        self.samples = []
        self.labels = []
        
        # æ ‡å‡†åŒ–ç­–ç•¥ï¼šæ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„ç»Ÿè®¡é‡è®¡ç®—æ–¹å¼
        if mode == 'full':
            print(f"[MFPT - å…¨æ•°æ®é›†æ¨¡å¼] ç›®å½•: {self.root_dir}")
            self.dataset_mean, self.dataset_std = self._compute_dataset_stats()
            print(f"æ•°æ®é›†ç»Ÿè®¡é‡: Î¼={self.dataset_mean:.6f}, Ïƒ={self.dataset_std:.6f}")
        elif mode == 'train':
            print(f"[MFPT - è®­ç»ƒé›†æ¨¡å¼] ç›®å½•: {self.root_dir}")
            self.dataset_mean, self.dataset_std = self._compute_dataset_stats()
            print(f"è®­ç»ƒé›†ç»Ÿè®¡é‡: Î¼={self.dataset_mean:.6f}, Ïƒ={self.dataset_std:.6f}")
        elif mode == 'test':
            if normalization_stats is None:
                raise ValueError("æµ‹è¯•æ¨¡å¼(mode='test')å¿…é¡»æä¾›normalization_statså‚æ•°")
            print(f"[MFPT - æµ‹è¯•é›†æ¨¡å¼] ç›®å½•: {self.root_dir}")
            self.dataset_mean = normalization_stats['mean']
            self.dataset_std = normalization_stats['std']
            print(f"ä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡: Î¼={self.dataset_mean:.6f}, Ïƒ={self.dataset_std:.6f}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'full', 'train' æˆ– 'test'")
        
        self._load_data()

    def _compute_dataset_stats(self) -> tuple:
        """è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„å…¨å±€ç»Ÿè®¡é‡ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰"""
        mat_files = sorted(list(self.root_dir.glob('*.mat')))
        if not mat_files:
            raise FileNotFoundError(f"åœ¨ {self.root_dir} ä¸­æœªæ‰¾åˆ°.matæ–‡ä»¶")
        
        print(f"æ‰«æ {len(mat_files)} ä¸ªæ–‡ä»¶ï¼Œè®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        all_signals = []
        
        for file_path in tqdm(mat_files, desc="æ”¶é›†æ•°æ®", leave=False):
            signal = load_mfpt_signal(str(file_path))
            if signal.size > 0:
                all_signals.append(signal)
        
        if not all_signals:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
        
        # åˆå¹¶æ‰€æœ‰ä¿¡å·å¹¶è®¡ç®—ç»Ÿè®¡é‡
        all_data = np.concatenate(all_signals)
        mean = all_data.mean()
        std = all_data.std() + 1e-6  # é˜²æ­¢é™¤é›¶
        
        return mean, std

    def _get_label_from_filename(self, filename: str) -> int:
        """æ ¹æ®æ–‡ä»¶åè§£ææ ‡ç­¾"""
        name = filename.lower()
        if 'baseline' in name:
            return self.LABEL_MAP['normal']
        if 'innerracefault' in name or 'inner' in name:
            return self.LABEL_MAP['inner_race']
        if 'outerracefault' in name or 'outer' in name:
            return self.LABEL_MAP['outer_race']
        return -1

    def _load_data(self):
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†"""
        mat_files = sorted(list(self.root_dir.glob('*.mat')))
        print(f"åŠ è½½ {len(mat_files)} ä¸ª.matæ–‡ä»¶...")

        for file_path in tqdm(mat_files, desc=f"åŠ è½½æ•°æ®({self.mode})", leave=False):
            label = self._get_label_from_filename(file_path.name)
            if label == -1:
                print(f"è­¦å‘Š: æ— æ³•è¯†åˆ«æ–‡ä»¶ {file_path.name} çš„æ ‡ç­¾ï¼Œå·²è·³è¿‡")
                continue

            signal = load_mfpt_signal(str(file_path))
            if signal.size < self.window_size:
                print(f"è­¦å‘Š: æ–‡ä»¶ {file_path.name} ä¿¡å·è¿‡çŸ­ ({signal.size})ï¼Œå·²è·³è¿‡")
                continue

            # ä½¿ç”¨æ•°æ®é›†çº§åˆ«çš„ç»Ÿè®¡é‡è¿›è¡Œæ ‡å‡†åŒ–
            num_windows = (len(signal) - self.window_size) // self.step_size + 1
            for i in range(num_windows):
                start_idx = i * self.step_size
                segment = signal[start_idx: start_idx + self.window_size]
                # å…³é”®ï¼šä½¿ç”¨æ•°æ®é›†ç»Ÿè®¡é‡ï¼Œè€Œéæ–‡ä»¶æˆ–çª—å£ç»Ÿè®¡é‡
                segment = (segment - self.dataset_mean) / self.dataset_std
                self.samples.append(segment)
                self.labels.append(label)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼å…±ç”Ÿæˆ {len(self.samples)} ä¸ªæ ·æœ¬")
        label_counts = {name: self.labels.count(idx) for name, idx in self.LABEL_MAP.items()}
        print(f"å„ç±»åˆ«æ ·æœ¬æ•°: {label_counts}")

    def get_normalization_stats(self) -> Dict:
        """è¿”å›æ ‡å‡†åŒ–ç»Ÿè®¡é‡ï¼Œä¾›æµ‹è¯•é›†ä½¿ç”¨"""
        return {
            'mean': self.dataset_mean,
            'std': self.dataset_std
        }

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        signal = torch.from_numpy(self.samples[idx]).float().unsqueeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        if self.augmentor:
            view1 = self.augmentor(signal)
            view2 = self.augmentor(signal)
            return view1, view2, label
        
        return signal, signal, label

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == '__main__':
    DATA_ROOT = r'E:\AI\MFPT-Fault-Data-Sets-20200227T131140Z-001\MFPT Fault Data Sets\MFPT'
    
    data_path = Path(DATA_ROOT)
    if not data_path.exists():
        print(f"âš ï¸  ç¤ºä¾‹ç›®å½• '{DATA_ROOT}' ä¸å­˜åœ¨")
        print("è¯·ä¿®æ”¹ DATA_ROOT ä¸ºä½ çš„å®é™…æ•°æ®è·¯å¾„")
        exit(0)
    
    print("=" * 80)
    print("åœºæ™¯1: å…¨æ•°æ®é›†æ¨¡å¼ - æ¨èç”¨äºæ— ç›‘ç£èšç±»ä¸»å®éªŒ")
    print("=" * 80)
    
    # å…¨æ•°æ®é›†æ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°
    full_dataset = MFPTDataset(
        root_dir=DATA_ROOT,
        window_size=1024,
        step_size=512,
        mode='full'  # å…³é”®å‚æ•°
    )
    
    print(f"\nâœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
    print(f"   æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
    print(f"   ç±»åˆ«æ˜ å°„: {full_dataset.LABEL_MAP}")
    
    # æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬
    sample_signal, _, sample_label = full_dataset[0]
    print(f"   æ ·æœ¬å½¢çŠ¶: {sample_signal.shape}")
    print(f"   æ ·æœ¬æ ‡ç­¾: {sample_label.item()} ({full_dataset.class_names[sample_label.item()]})")
    
    # æµ‹è¯•DataLoader
    from torch.utils.data import DataLoader
    loader = DataLoader(full_dataset, batch_size=64, shuffle=True)
    batch = next(iter(loader))
    print(f"   æ‰¹æ¬¡å½¢çŠ¶: {batch[0].shape}")
    
    print("\nğŸ’¡ ç”¨é€”: å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ + å…¨æ•°æ®èšç±»è¯„ä¼°")
    print("   ç¤ºä¾‹ä»£ç :")
    print("   >>> model.fit(full_dataset)")
    print("   >>> evaluate_clustering(full_dataset)")
    
    print("\n" + "=" * 80)
    print("åœºæ™¯2: è®­ç»ƒ/æµ‹è¯•æ¨¡å¼ - ç”¨äºæ¶ˆèå®éªŒï¼Œè¯„ä¼°æ³›åŒ–èƒ½åŠ›")
    print("=" * 80)
    print("âš ï¸  éœ€è¦å…ˆå°†æ•°æ®åˆ’åˆ†ä¸ºtrainå’Œtestç›®å½•")
    print("   å¯ä»¥ä½¿ç”¨ utils/split_dataset.py å·¥å…·è‡ªåŠ¨åˆ’åˆ†")
    
    # å‡è®¾å·²ç»åˆ’åˆ†å¥½æ•°æ®
    TRAIN_DIR = DATA_ROOT  # å®é™…ä½¿ç”¨æ—¶æ”¹ä¸º: DATA_ROOT + '/train'
    TEST_DIR = DATA_ROOT   # å®é™…ä½¿ç”¨æ—¶æ”¹ä¸º: DATA_ROOT + '/test'
    
    # è®­ç»ƒé›†
    print("\n[1] åŠ è½½è®­ç»ƒé›†...")
    train_dataset = MFPTDataset(
        root_dir=TRAIN_DIR,
        window_size=1024,
        step_size=512,
        mode='train'  # è®­ç»ƒæ¨¡å¼
    )
    
    # è·å–å¹¶ä¿å­˜ç»Ÿè®¡é‡
    stats = train_dataset.get_normalization_stats()
    print(f"   è®­ç»ƒé›†ç»Ÿè®¡é‡: {stats}")
    
    # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    print("\n[2] åŠ è½½æµ‹è¯•é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡ï¼‰...")
    # test_dataset = MFPTDataset(
    #     root_dir=TEST_DIR,
    #     window_size=1024,
    #     step_size=512,
    #     mode='test',
    #     normalization_stats=stats  # å…³é”®ï¼šä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡
    # )
    
    print("\nğŸ’¡ ç”¨é€”: è¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šçš„èšç±»æ€§èƒ½")
    print("   ç¤ºä¾‹ä»£ç :")
    print("   >>> model.fit(train_dataset)")
    print("   >>> evaluate_clustering(test_dataset)")
    
    print("\n" + "=" * 80)
    print("åœºæ™¯3: å¯¹æ¯”å­¦ä¹ æ•°æ®å¢å¼º")
    print("=" * 80)
    
    # å®šä¹‰ç®€å•çš„æ•°æ®å¢å¼ºå™¨
    class SimpleAugmentor:
        def __call__(self, signal):
            # æ·»åŠ é«˜æ–¯å™ªå£°
            noise = torch.randn_like(signal) * 0.01
            return signal + noise
    
    augmented_dataset = MFPTDataset(
        root_dir=DATA_ROOT,
        window_size=1024,
        step_size=512,
        mode='full',
        augmentor=SimpleAugmentor()  # å¯ç”¨æ•°æ®å¢å¼º
    )
    
    # __getitem__ ä¼šè¿”å›ä¸¤ä¸ªå¢å¼ºè§†å›¾
    view1, view2, label = augmented_dataset[0]
    print(f"âœ… å¢å¼ºåè¿”å›ä¸¤ä¸ªè§†å›¾:")
    print(f"   View1 å½¢çŠ¶: {view1.shape}")
    print(f"   View2 å½¢çŠ¶: {view2.shape}")
    print(f"   æ ‡ç­¾: {label.item()}")
    
    print("\nğŸ’¡ ç”¨é€”: è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ")
    print("   ç¤ºä¾‹ä»£ç :")
    print("   >>> loss = contrastive_loss(encoder(view1), encoder(view2))")
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ€»ç»“")
    print("=" * 80)
    print("âœ… ä¸»å®éªŒï¼ˆæ— ç›‘ç£èšç±»ï¼‰: ä½¿ç”¨ mode='full'")
    print("âœ… æ¶ˆèå®éªŒï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰: ä½¿ç”¨ mode='train' å’Œ mode='test'")
    print("âœ… å¯¹æ¯”å­¦ä¹ : ä¼ å…¥ augmentor å‚æ•°")
    print("=" * 80)



